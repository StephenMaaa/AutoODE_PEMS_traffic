{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0dc8f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "import time\n",
    "from DNN import Seq2Seq, LWRDataset, LWRDataset_res, train_LWR, eval_LWR, test_LWR, train_hybrid_LWR, eval_hybrid_LWR, test_hybrid_LWR \n",
    "from AutoODE import LWR_batch_version\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7742dd0",
   "metadata": {},
   "source": [
    "### Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8189f3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data = torch.load(\"data_LWR/residual/training_time.pt\").double()\n",
    "# training_initial = torch.load(\"data_LWR/residual/training_time_initial.pt\").double()\n",
    "# training_boundary = torch.load(\"data_LWR/residual/training_time_boundary.pt\").double()\n",
    "# x_train = torch.load(\"data_LWR/residual/x_train.pt\").long()\n",
    "# test_data = torch.load(\"data_LWR/residual/test_time.pt\").double()\n",
    "# test_initial = torch.load(\"data_LWR/residual/test_time_initial.pt\").double()\n",
    "# test_boundary = torch.load(\"data_LWR/residual/test_time_boundary.pt\").double()\n",
    "# x_test = torch.load(\"data_LWR/residual/x_test.pt\").long() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba06507a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data = torch.load(\"data_LWR/space/training_time.pt\").double()\n",
    "# training_initial = torch.load(\"data_LWR/space/training_time_initial.pt\").double()\n",
    "# training_boundary = torch.load(\"data_LWR/space/training_time_boundary.pt\").double() \n",
    "# test_data = torch.load(\"data_LWR/space/test_time.pt\").double()\n",
    "# test_initial = torch.load(\"data_LWR/space/test_time_initial.pt\").double()\n",
    "# test_boundary = torch.load(\"data_LWR/space/test_time_boundary.pt\").double() \n",
    "# x_train = torch.load(\"data_LWR/space/x_train.pt\").long()\n",
    "# x_test = torch.load(\"data_LWR/space/x_test.pt\").long() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "625d1982",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = torch.load(\"../pems_I5_S_correct.pt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f31769f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xi = [0,  45,  56,  75,  81,  86,  89,  95, 100, 105, 109, 112, 117,\n",
    "       124, 128, 133, 137, 141, 146, 149, 152, 158, 163, 167, 171, 174,\n",
    "       180, 186, 192, 197, 200, 205, 207, 210, 211, 213, 214, 228, 231,\n",
    "       237, 240, 242, 251, 254, 258, 262, 266, 270, 277, 279, 282, 283,\n",
    "       286, 288, 291, 294, 296, 298, 300, 303, 308, 310, 315, 317, 320,\n",
    "       322, 327, 338, 342, 345, 352, 356, 359, 362, 366, 368, 374, 379] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8af5fa3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 200 650\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#nx=350  # \n",
    "nx=380 \n",
    "#kj needs to be larger than k for the solution to be stable \n",
    "# kj = np.ones((nx,), dtype=float) * 0.6\n",
    "# kj = (kj - mean[0].numpy()) / std[0].numpy()\n",
    "# kj = (kj - y_min[0].numpy()) / (y_max[0].numpy() - y_min[0].numpy()) # normalize\n",
    "\n",
    "#characteristic velocity (m/s), corresponds to roughly 120 km/h\n",
    "# vf = np.ones((nx,), dtype=float) * 38\n",
    "# vf = (vf - mean[2].numpy()) / std[2].numpy()\n",
    "# vf = (vf - y_min[2].numpy()) / (y_max[2].numpy() - y_min[2].numpy()) # normalize\n",
    "\n",
    "dx=300.\n",
    "\n",
    "## change the timestep to dt = 1, previously dt = 6 with 7 mins runtime\n",
    "dt=6\n",
    "#need an output every 5 mins (300 s), so tskip = 3 with dt = 3s\n",
    "tskip=50\n",
    "#nt=int(3600*6/6 - 50)\n",
    "#nt=7099 #6 hours (times 3600 s/hour divided by dt=3s)\n",
    "nto=4 + 1 \n",
    "#nt=int(3600*nto/12/6/dt - tskip)\n",
    "dtobs=300\n",
    "nt=int((dtobs*nto)/dt - tskip) \n",
    "\n",
    "nto=13 + 1 \n",
    "#nt=int(3600*nto/12/6/dt - tskip)\n",
    "dtobs=300\n",
    "nt_test=int((dtobs*nto)/dt - tskip)\n",
    "print(dt, nt, nt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8a0fa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sensors = np.array(([60,  6, 58, 53, 45,  1, 72, 52, 48, 55, 18, 22, 64,  4, 65,  5, 30,\n",
    "       46, 66, 23, 49, 35, 17, 20, 33, 68, 77, 69, 56, 67, 42, 54, 39, 71,\n",
    "       61,  2, 59, 43, 32, 70, 12, 10, 47, 28, 37, 74, 40,  7, 25,  8, 57,\n",
    "        9, 34, 63, 26, 51, 21, 44]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f71db422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  4,  5,  6,  7,  8,  9, 10, 12, 17, 18, 20, 21, 22, 23, 25,\n",
       "       26, 28, 30, 32, 33, 34, 35, 37, 39, 40, 42, 43, 44, 45, 46, 47, 48,\n",
       "       49, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67,\n",
       "       68, 69, 70, 71, 72, 74, 77])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_sensors.sort() \n",
    "# training_sensors = training_sensors - 1 \n",
    "training_sensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "204111e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  3, 11, 13, 14, 15, 16, 19, 24, 27, 29, 31, 36, 38, 41, 50, 62,\n",
       "       73, 75, 76])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sensors = np.array(list(set(np.arange(0, 78)) - set(training_sensors))) \n",
    "test_sensors.sort() \n",
    "test_sensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f4df69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def interpolate_initial(var, xi, t0=0): \n",
    "    IM_q=interp1d(np.array(xi) * dx, var[1, :].numpy(), bounds_error=False, \n",
    "                fill_value=(var[1, 0], var[1, -1]), kind='linear') \n",
    "    IM_u=interp1d(np.array(xi) * dx, var[2, :].numpy(), bounds_error=False, \n",
    "                fill_value=(var[2, 0], var[2, -1]), kind='linear')\n",
    "\n",
    "    x=np.linspace(0, (nx-1) * dx, nx) \n",
    "    u = IM_u(x)\n",
    "    q = IM_q(x)\n",
    "    k = q / u\n",
    "    initial = np.stack((k, q, u))\n",
    "    return torch.tensor(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "764173ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ti=np.arange(0, (nt-1)*dt, tskip*dt )\n",
    "t=np.linspace(0, (nt-1)*dt, nt) \n",
    "ti_test=np.arange(0, (nt_test-1)*dt, tskip*dt )\n",
    "t_test = np.linspace(0, (nt_test-1)*dt, nt_test)\n",
    "\n",
    "def interpolate_boundary(var, ti, t): \n",
    "    IM_q=interp1d(np.array(ti), var[1, :, 0].numpy(), bounds_error=False,\n",
    "             fill_value=(var[1, 0, 0], var[1, -1, 0]), kind='linear')\n",
    "    IM_u=interp1d(np.array(ti), var[2, :, 0].numpy(), bounds_error=False,\n",
    "             fill_value=(var[2, 0, 0], var[2, -1, 0]), kind='linear')\n",
    "    u = IM_u(t)\n",
    "    q = IM_q(t)\n",
    "    k = q / u\n",
    "    boundary = np.stack((k, q, u)) \n",
    "    return torch.tensor(boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3767dbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 53280, 380])\n"
     ]
    }
   ],
   "source": [
    "sensor_idx = list(np.arange(380)) \n",
    "\n",
    "def generate_data_lwr_time(data): \n",
    "    interpolated = []\n",
    "    for i in range(data.shape[1]): \n",
    "        interpolated_d = interpolate_initial(data[:, i, torch.tensor(training_sensors).long()], list(np.array(xi)[training_sensors]), t0=0) \n",
    "        interpolated.append(interpolated_d) \n",
    "    interpolated_data = torch.stack(interpolated) \n",
    "    interpolated_data = interpolated_data.permute(1, 0, 2)\n",
    "    print(interpolated_data.shape) \n",
    "    \n",
    "    training_set = [] \n",
    "    test_set = [] \n",
    "    initial_train = []\n",
    "    boundary_train = []\n",
    "    initial_test = []\n",
    "    boundary_test = []\n",
    "    x_train = []\n",
    "    x_test = []\n",
    "    training_size = int((data.shape[1] - 13) * 0.8) \n",
    "    \n",
    "    for i in range(data.shape[1] - 13): \n",
    "#         time_train.append(torch.tensor((i % 288) // 24)) \n",
    "        x_train.append(torch.tensor(sensor_idx))\n",
    "        training_set.append(interpolated_data[:, i:i+4, :])\n",
    "#         x_train.append(torch.tensor(list(np.array(xi)[training_sensors])).long())\n",
    "#         training_set.append(data[:, i:i+4, training_sensors])\n",
    "        initial = interpolate_initial(data[:, i, :], xi, t0=0)\n",
    "        boundary = interpolate_boundary(data[:, i:i+4, :], ti, t) \n",
    "#             print(initial.shape)\n",
    "#             print(boundary.shape)\n",
    "        initial_train.append(initial)\n",
    "        boundary_train.append(boundary)\n",
    "\n",
    "#         time_test.append(torch.tensor((i % 288) // 24)) \n",
    "        x_test.append(torch.tensor(np.array(xi)[test_sensors]))\n",
    "        test_set.append(data[:, i:i+13, torch.tensor(test_sensors).long()]) \n",
    "#         x_test.append(torch.tensor(list(np.array(xi)[test_sensors])).long())\n",
    "#         test_set.append(data[:, i:i+12, test_sensors]) \n",
    "        initial = interpolate_initial(data[:, i, :], xi, t0=0)\n",
    "        boundary = interpolate_boundary(data[:, i:i+13, :], ti_test, t_test)\n",
    "        initial_test.append(initial)\n",
    "        boundary_test.append(boundary)\n",
    "    return torch.stack(x_train), torch.stack(x_test), torch.stack(training_set), torch.stack(test_set), torch.stack(initial_train), torch.stack(initial_test), torch.stack(boundary_train), torch.stack(boundary_test)   \n",
    "\n",
    "x_train, x_test, training_set, test_set, initial_train, initial_test, boundary_train, boundary_test = generate_data_lwr_time(data1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65800eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(time_train, \"data_LWR/time_train.pt\") \n",
    "# torch.save(time_test, \"data_LWR/time_test.pt\")\n",
    "torch.save(training_set, \"data_LWR/space/training_space.pt\")\n",
    "torch.save(test_set, \"data_LWR/space/test_space.pt\")\n",
    "torch.save(initial_train, \"data_LWR/space/training_space_initial.pt\")\n",
    "torch.save(initial_test, \"data_LWR/space/test_space_initial.pt\")\n",
    "torch.save(boundary_train, \"data_LWR/space/training_space_boundary.pt\")\n",
    "torch.save(boundary_test, \"data_LWR/space/test_space_boundary.pt\")\n",
    "torch.save(x_train, \"data_LWR/space/x_train.pt\")\n",
    "torch.save(x_test, \"data_LWR/space/x_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03cf67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = LWRDataset(x_train, training_set, initial_train, boundary_train)\n",
    "test_set = LWRDataset(x_test, test_set, initial_test, boundary_test)\n",
    "training_set, val_set = data.random_split(training_set, [int(len(training_set) * 0.875), int(len(training_set) - int(len(training_set) * 0.875))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa9e0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(training_set, batch_size = 256, shuffle = True, num_workers=0, pin_memory=True)\n",
    "val_loader = data.DataLoader(val_set, batch_size = 512, shuffle = False, num_workers=0, pin_memory=True)\n",
    "test_loader = data.DataLoader(test_set, batch_size = 512, shuffle = False, num_workers=0, pin_memory=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372a1ab8",
   "metadata": {},
   "source": [
    "### Hyperparameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0f7ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d \n",
    "\n",
    "def calibrate(data): \n",
    "    # assume the linear relation u = a * k + b \n",
    "    # cross all sensors / at each sensor \n",
    "#     print(data.shape)\n",
    "    kj = data[0, :42613].max(dim = 0).values\n",
    "    vf = data[2, :42613].max(dim = 0).values \n",
    "    IM_kj=interp1d(np.array(xi) * dx, kj.numpy(), bounds_error=False, \n",
    "                fill_value=(kj[0], kj[-1]), kind='linear') \n",
    "    IM_vf=interp1d(np.array(xi) * dx, vf.numpy(), bounds_error=False, \n",
    "                fill_value=(vf[0], vf[-1]), kind='linear')\n",
    "\n",
    "    x=np.linspace(0, (nx-1) * dx, nx) \n",
    "    kj = IM_kj(x) \n",
    "    vf = IM_vf(x) \n",
    "#     kj = (kj - y_min[0].numpy()) / (y_max[0].numpy() - y_min[0].numpy()) \n",
    "#     vf = (vf - y_min[2].numpy()) / (y_max[2].numpy() - y_min[2].numpy())\n",
    "#     k_mean = data[0].mean(dim = 0)\n",
    "#     u_mean = data[2].mean(dim = 0) \n",
    "#     k_m = data[0] - k_mean\n",
    "    \n",
    "#     b = ((k_m) * (data[2] - u_mean)).sum(dim = 0) / (k_m * k_m).sum(dim = 0)\n",
    "# #     print(b.shape)\n",
    "# #     a = u_mean - b * k_mean # vf\n",
    "#     kj = -(u_max / b)\n",
    "#     return kj, a   \n",
    "    return kj * 2, np.ceil(vf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8374af83",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = torch.load(\"../pems_I5_S_correct.pt\") \n",
    "kj, vf = calibrate(data2) \n",
    "# kj, vf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc038d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps=torch.tensor(np.linspace(0, nt, nt), requires_grad=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cb4a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_test=torch.tensor(np.linspace(0, nt_test, nt_test), requires_grad=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31f3508",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" \n",
    "# model = LWR_seq2seq(LWR_model).to(device) \n",
    "model = LWR_batch_version(nx, 300, 6, kj, vf, tskip, plm = False, plm_vf = False, \n",
    "                          initial={}, boundary={}, fix_vf=False, parstep=1).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d641784",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"LWR\" \n",
    "learning_rate = 0.0015 \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay = 0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 1, gamma=0.95)\n",
    "criterion = nn.MSELoss()\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "best_loss = 100   \n",
    "train_losses = []\n",
    "val_losses = []\n",
    "tsteps = steps.shape[0]\n",
    "num_epoch = 100 \n",
    "trial = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c110b892",
   "metadata": {},
   "source": [
    "### Training and Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39482304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "for epoch in range(1, num_epoch + 1): \n",
    "    start = time.time()\n",
    "    train_loss = train_LWR(model, train_loader, optimizer, criterion, tsteps)[-1]\n",
    "    train_losses.append(train_loss)\n",
    "    _, _, val_loss = eval_LWR(model, val_loader, criterion, tsteps) \n",
    "\n",
    "    val_losses.append(val_loss)\n",
    "    if val_loss <= best_loss: \n",
    "        best_loss = val_loss \n",
    "        best_model = model\n",
    "        torch.save({\"lr\": optimizer.param_groups[0]['lr'], \"model\": model}, \"best3_AutoODE\" + str(trial) + \".pt\")\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Epoch:\", epoch, \"completed in:\", (end - start), \"s. Training loss:\", train_loss, \". Val loss:\", val_loss)  \n",
    "    if (len(train_losses) > 30 and np.mean(val_losses[-5:]) >= np.mean(val_losses[-10:-5])):\n",
    "        break\n",
    "    scheduler.step() \n",
    "    if epoch % 5 == 0: print(optimizer.param_groups[0]['lr']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c81b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsteps_test = steps_test.shape[0] \n",
    "preds, trues, test_loss = test_LWR(model, test_loader, criterion, tsteps_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5fd7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, np.sqrt(test_loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba0c194",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"preds\": preds, \"trues\": trues, \"model\": model}, \"new_result/final_AutoODE3.pt\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
