{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9997bf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "import time\n",
    "from DNN import Dataset, train_epoch, eval_epoch, eval_epoch_true, FC, Seq2Seq, Seq2Seq_Attn \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a3ddf2",
   "metadata": {},
   "source": [
    "### Data for time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae599ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data = torch.load(\"data_time/pems_bay/training_time.pt\").float()\n",
    "# test_data = torch.load(\"data_time/pems_bay/test_time.pt\").float()\n",
    "# training_data = torch.load(\"data_time/pems_sd/training_time.pt\").float()\n",
    "# test_data = torch.load(\"data_time/pems_sd/test_time.pt\").float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d429d123",
   "metadata": {},
   "source": [
    "### Data for space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98169690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data = torch.load(\"data_space/training_space.pt\").float()\n",
    "# test_data = torch.load(\"data_space/test_space.pt\").float()\n",
    "training_data = torch.load(\"data_space/training_space_region.pt\").float()\n",
    "test_data = torch.load(\"data_space/test_space_region.pt\").float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30843848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([159768, 3, 24, 20]), torch.Size([53256, 3, 24, 20]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.shape, test_data.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5f06d8",
   "metadata": {},
   "source": [
    "### Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "923e1df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.tensor([training_data[:, 0, :, :].mean(), training_data[:, 1, :, :].mean(), training_data[:, 2, :, :].mean()])\n",
    "std = torch.tensor([training_data[:, 0, :, :].std(), training_data[:, 1, :, :].std(), training_data[:, 2, :, :].std()])\n",
    "training_norm = torch.zeros(training_data.shape, dtype = torch.float) \n",
    "training_norm[:, 0, :, :] = (training_data[:, 0, :, :] - mean[0]) / std[0]\n",
    "training_norm[:, 1, :, :] = (training_data[:, 1, :, :] - mean[1]) / std[1]\n",
    "training_norm[:, 2, :, :] = (training_data[:, 2, :, :] - mean[2]) / std[2] \n",
    "\n",
    "test_norm = torch.zeros(test_data.shape, dtype = torch.float) \n",
    "test_norm[:, 0, :, :] = (test_data[:, 0, :, :] - mean[0]) / std[0]\n",
    "test_norm[:, 1, :, :] = (test_data[:, 1, :, :] - mean[1]) / std[1]\n",
    "test_norm[:, 2, :, :] = (test_data[:, 2, :, :] - mean[2]) / std[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "511eeb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = Dataset(training_norm)\n",
    "test_set = Dataset(test_norm) \n",
    "training_set, val_set = data.random_split(training_set, [int(len(training_set) * 0.875), int(len(training_set) - int(len(training_set) * 0.875))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a61796bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader \n",
    "train_loader = data.DataLoader(training_set, batch_size = 512, shuffle = True)\n",
    "val_loader = data.DataLoader(val_set, batch_size = 512, shuffle = False) \n",
    "test_loader = data.DataLoader(test_set, batch_size = 512, shuffle = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2b0b282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139797\n",
      "19971\n",
      "53256\n"
     ]
    }
   ],
   "source": [
    "print(len(training_set))\n",
    "print(len(val_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af75743",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19ba19ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2381884\n",
      "------Trial 1\n",
      "Epoch: 1 completed in: 6.517144203186035 s. Training loss: 0.1689617785006544 . Val loss: 0.11405108403414488 . Test loss: 1.2035742965275702\n",
      "Epoch: 2 completed in: 6.261345148086548 s. Training loss: 0.09778780441214568 . Val loss: 0.10099496580660343 . Test loss: 1.2962989793447937\n",
      "Epoch: 3 completed in: 6.663124322891235 s. Training loss: 0.08843313044712056 . Val loss: 0.08567229378968477 . Test loss: 1.250748950837013\n",
      "Epoch: 4 completed in: 6.607799291610718 s. Training loss: 0.07983345867399752 . Val loss: 0.07891990896314383 . Test loss: 1.298827730391156\n",
      "Epoch: 5 completed in: 5.840408563613892 s. Training loss: 0.07477584742281558 . Val loss: 0.07791995331645012 . Test loss: 1.3362691245651257\n",
      "0.0038689046874999995\n",
      "Epoch: 6 completed in: 5.999856948852539 s. Training loss: 0.07051301293455771 . Val loss: 0.07225849609822035 . Test loss: 1.3516470284441156\n",
      "Epoch: 7 completed in: 5.876299142837524 s. Training loss: 0.06640078048527676 . Val loss: 0.0688509039580822 . Test loss: 1.398817856907783\n",
      "Epoch: 8 completed in: 6.311426162719727 s. Training loss: 0.06154395077024063 . Val loss: 0.06460915561765432 . Test loss: 1.3429656653531414\n",
      "Epoch: 9 completed in: 6.188684940338135 s. Training loss: 0.059027238986896775 . Val loss: 0.06271455883979797 . Test loss: 1.3708066879674903\n",
      "Epoch: 10 completed in: 6.0463926792144775 s. Training loss: 0.05512068243901225 . Val loss: 0.059475218132138255 . Test loss: 1.43006477872209\n",
      "0.002993684696191893\n",
      "Epoch: 11 completed in: 5.912450551986694 s. Training loss: 0.05260449692770077 . Val loss: 0.05622494881972671 . Test loss: 1.3711007310179564\n",
      "Epoch: 12 completed in: 6.086760997772217 s. Training loss: 0.04908411943074996 . Val loss: 0.05322370417416096 . Test loss: 1.4406798084457613\n",
      "Epoch: 13 completed in: 5.838599443435669 s. Training loss: 0.04639425461799124 . Val loss: 0.05075410474091768 . Test loss: 1.5152992205999822\n",
      "Epoch: 14 completed in: 5.930679798126221 s. Training loss: 0.04403958835360343 . Val loss: 0.04808435142040253 . Test loss: 1.4658028299217276\n",
      "Epoch: 15 completed in: 6.240833044052124 s. Training loss: 0.042219295356776154 . Val loss: 0.04585406444966793 . Test loss: 1.475488320627023\n",
      "0.002316456150798765\n",
      "Epoch: 16 completed in: 6.46274471282959 s. Training loss: 0.039996951410587685 . Val loss: 0.04406277695670724 . Test loss: 1.530005006584957\n",
      "Epoch: 17 completed in: 6.245793342590332 s. Training loss: 0.03812077033748157 . Val loss: 0.04199330788105726 . Test loss: 1.5257804163247652\n",
      "Epoch: 18 completed in: 6.098410129547119 s. Training loss: 0.036823493788820987 . Val loss: 0.04178573526442051 . Test loss: 1.5665380937251507\n",
      "Epoch: 19 completed in: 6.481566667556763 s. Training loss: 0.035963413264792764 . Val loss: 0.03874982250854373 . Test loss: 1.541887489297866\n",
      "Epoch: 20 completed in: 6.144855737686157 s. Training loss: 0.03415938640815498 . Val loss: 0.038276039715856316 . Test loss: 1.5294324777987167\n",
      "0.0017924296120427094\n",
      "Epoch: 21 completed in: 6.086504697799683 s. Training loss: 0.03344905639515958 . Val loss: 0.03703924575820565 . Test loss: 1.5382411521278594\n",
      "Epoch: 22 completed in: 5.942885875701904 s. Training loss: 0.032146501905509155 . Val loss: 0.03663939945399761 . Test loss: 1.532246780482092\n",
      "Epoch: 23 completed in: 6.2371985912323 s. Training loss: 0.031481521238103834 . Val loss: 0.034744682861492036 . Test loss: 1.5240507507325742\n",
      "Epoch: 24 completed in: 6.270983695983887 s. Training loss: 0.030577699905329377 . Val loss: 0.03383215544745326 . Test loss: 1.50059083606425\n",
      "Epoch: 25 completed in: 6.121574878692627 s. Training loss: 0.029595167539252416 . Val loss: 0.033256253646686676 . Test loss: 1.5395188561206885\n",
      "0.0013869478656091687\n",
      "Epoch: 26 completed in: 5.960703611373901 s. Training loss: 0.02950062290731355 . Val loss: 0.03285840558819473 . Test loss: 1.5681464761467654\n",
      "Epoch: 27 completed in: 6.07339072227478 s. Training loss: 0.02863855702788943 . Val loss: 0.03222658396698534 . Test loss: 1.5700321699202813\n",
      "Epoch: 28 completed in: 6.299288749694824 s. Training loss: 0.028294475912286416 . Val loss: 0.033126400457695125 . Test loss: 1.5437330842860992\n",
      "Epoch: 29 completed in: 6.09632420539856 s. Training loss: 0.027704228538285643 . Val loss: 0.031049686064943672 . Test loss: 1.5296161938827837\n",
      "Epoch: 30 completed in: 5.88026762008667 s. Training loss: 0.026960324468838906 . Val loss: 0.030437098257243632 . Test loss: 1.5223045843737608\n",
      "0.0010731938197146862\n",
      "Epoch: 31 completed in: 6.054819583892822 s. Training loss: 0.026533092324533603 . Val loss: 0.030110795982182026 . Test loss: 1.5702533695427354\n",
      "Epoch: 32 completed in: 5.8728721141815186 s. Training loss: 0.02605018350737591 . Val loss: 0.029725918220356105 . Test loss: 1.5546038820336376\n",
      "Epoch: 33 completed in: 6.2504189014434814 s. Training loss: 0.02550913056317907 . Val loss: 0.029183110129088162 . Test loss: 1.5757766951240204\n",
      "Epoch: 34 completed in: 6.424346923828125 s. Training loss: 0.024987247915272296 . Val loss: 0.028846120880916714 . Test loss: 1.5650959659225858\n",
      "Epoch: 35 completed in: 6.733954429626465 s. Training loss: 0.02460599080897378 . Val loss: 0.02849490479566157 . Test loss: 1.5554636844942018\n",
      "0.0008304169199380356\n",
      "Epoch: 36 completed in: 6.426443099975586 s. Training loss: 0.024321795094513547 . Val loss: 0.028358645224943756 . Test loss: 1.5547840268725588\n",
      "Epoch: 37 completed in: 6.764234781265259 s. Training loss: 0.023987057469241374 . Val loss: 0.028006377117708325 . Test loss: 1.551386979432747\n",
      "Epoch: 38 completed in: 6.83220100402832 s. Training loss: 0.02376942819895318 . Val loss: 0.02780563309788704 . Test loss: 1.554871256681415\n",
      "Epoch: 39 completed in: 6.3623785972595215 s. Training loss: 0.023489754714996276 . Val loss: 0.027507093222811817 . Test loss: 1.5451120326271586\n",
      "Epoch: 40 completed in: 5.964779615402222 s. Training loss: 0.023114367653309865 . Val loss: 0.027084359573200344 . Test loss: 1.5728130677131689\n",
      "0.0006425607828255154\n",
      "Epoch: 41 completed in: 6.297496557235718 s. Training loss: 0.022896570459443287 . Val loss: 0.02704561180435121 . Test loss: 1.5741576127336434\n",
      "Epoch: 42 completed in: 6.284666538238525 s. Training loss: 0.022763647941233468 . Val loss: 0.026811780501157047 . Test loss: 1.5783214761080067\n",
      "Epoch: 43 completed in: 5.9274842739105225 s. Training loss: 0.022452212972090626 . Val loss: 0.026462758658453822 . Test loss: 1.5941154338401378\n",
      "Epoch: 44 completed in: 6.128972291946411 s. Training loss: 0.022229259249067653 . Val loss: 0.026279836473986507 . Test loss: 1.5896522143267993\n",
      "Epoch: 45 completed in: 6.260247468948364 s. Training loss: 0.022077139738919963 . Val loss: 0.026266934303566813 . Test loss: 1.5837562024430234\n",
      "0.000497201284935461\n",
      "Epoch: 46 completed in: 6.385105133056641 s. Training loss: 0.021905856023467805 . Val loss: 0.025971200549975038 . Test loss: 1.5604186841755354\n",
      "Epoch: 47 completed in: 6.021267414093018 s. Training loss: 0.021641254431846804 . Val loss: 0.025818367721512914 . Test loss: 1.5813084336043373\n",
      "Epoch: 48 completed in: 6.259433269500732 s. Training loss: 0.021473204438323085 . Val loss: 0.025749091943725943 . Test loss: 1.5903909312668967\n",
      "Epoch: 49 completed in: 6.154112339019775 s. Training loss: 0.02140195365913593 . Val loss: 0.025571612222120167 . Test loss: 1.5877774544301708\n",
      "Epoch: 50 completed in: 6.163340091705322 s. Training loss: 0.021251315645275326 . Val loss: 0.025438587320968507 . Test loss: 1.5795318580743682\n",
      "0.0003847248763835656\n",
      "Epoch: 51 completed in: 6.144651889801025 s. Training loss: 0.021104544632299972 . Val loss: 0.02528794608078897 . Test loss: 1.596640534260445\n",
      "Epoch: 52 completed in: 6.270450830459595 s. Training loss: 0.02102204173612986 . Val loss: 0.025206366367638112 . Test loss: 1.5895111063141052\n",
      "Epoch: 53 completed in: 6.0204758644104 s. Training loss: 0.02079640345879062 . Val loss: 0.025031460216268897 . Test loss: 1.603320114901993\n",
      "Epoch: 54 completed in: 6.335238695144653 s. Training loss: 0.020675216121667058 . Val loss: 0.024982444196939468 . Test loss: 1.5872734169338298\n",
      "Epoch: 55 completed in: 5.8660805225372314 s. Training loss: 0.020548740884520276 . Val loss: 0.02485787607729435 . Test loss: 1.5831821154175367\n",
      "0.000297692775527647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56 completed in: 6.114526987075806 s. Training loss: 0.020443981690128354 . Val loss: 0.024777042353525756 . Test loss: 1.5915039582455455\n",
      "Epoch: 57 completed in: 5.936382532119751 s. Training loss: 0.020358708095702813 . Val loss: 0.024708747677505017 . Test loss: 1.5876674500851486\n",
      "Epoch: 58 completed in: 6.253287315368652 s. Training loss: 0.020263028280116127 . Val loss: 0.024583966424688696 . Test loss: 1.5805127314476317\n",
      "Epoch: 59 completed in: 6.121990442276001 s. Training loss: 0.020165601330571367 . Val loss: 0.024511179607361554 . Test loss: 1.5941805807746372\n",
      "Epoch: 60 completed in: 6.404022455215454 s. Training loss: 0.020084849504386857 . Val loss: 0.024479192681610584 . Test loss: 1.59440438587442\n",
      "0.00023034899493475967\n",
      "Epoch: 61 completed in: 6.108110666275024 s. Training loss: 0.020007400379183082 . Val loss: 0.0243917650077492 . Test loss: 1.597734078143724\n",
      "Epoch: 62 completed in: 6.235682725906372 s. Training loss: 0.019894777341698207 . Val loss: 0.02430355395190418 . Test loss: 1.591251752692109\n",
      "Epoch: 63 completed in: 5.940013408660889 s. Training loss: 0.019843526238942668 . Val loss: 0.02427151701413095 . Test loss: 1.5971406214942467\n",
      "Epoch: 64 completed in: 6.169231414794922 s. Training loss: 0.019786926475863387 . Val loss: 0.024247942492365837 . Test loss: 1.5966427178725437\n",
      "Epoch: 65 completed in: 6.131382703781128 s. Training loss: 0.019731207832313368 . Val loss: 0.02415747274644673 . Test loss: 1.5974391302505295\n",
      "0.00017823966125280104\n",
      "Epoch: 66 completed in: 6.021229028701782 s. Training loss: 0.019611086669194436 . Val loss: 0.024128886125981807 . Test loss: 1.5922771819357535\n",
      "Epoch: 67 completed in: 6.361927509307861 s. Training loss: 0.01955606233002278 . Val loss: 0.02408816209062934 . Test loss: 1.5951432565446781\n",
      "Epoch: 68 completed in: 5.898913383483887 s. Training loss: 0.01950560204249664 . Val loss: 0.02405996681191027 . Test loss: 1.591959545572995\n",
      "Epoch: 69 completed in: 6.6038525104522705 s. Training loss: 0.019471355577944403 . Val loss: 0.024008757807314395 . Test loss: 1.6030399407944127\n",
      "Epoch: 70 completed in: 6.221521854400635 s. Training loss: 0.01943542418770329 . Val loss: 0.023962666885927318 . Test loss: 1.5976620564214783\n",
      "0.00013791845218387477\n",
      "Epoch: 71 completed in: 6.185837745666504 s. Training loss: 0.019360269807333495 . Val loss: 0.023920114198699594 . Test loss: 1.604319899963021\n",
      "Epoch: 72 completed in: 5.9944446086883545 s. Training loss: 0.019285264795469326 . Val loss: 0.023893139977008104 . Test loss: 1.6029373621530982\n",
      "Epoch: 73 completed in: 6.301706790924072 s. Training loss: 0.019244479674873124 . Val loss: 0.023868985148146747 . Test loss: 1.597793939537419\n",
      "Epoch: 74 completed in: 6.1575610637664795 s. Training loss: 0.01919821528106058 . Val loss: 0.02386122811585665 . Test loss: 1.600502179545343\n",
      "Epoch: 75 completed in: 6.31830358505249 s. Training loss: 0.019167655800217693 . Val loss: 0.02381517766043544 . Test loss: 1.6018851076911471\n",
      "0.00010671866922938751\n",
      "Epoch: 76 completed in: 6.212648153305054 s. Training loss: 0.019141276645725663 . Val loss: 0.023811784433200957 . Test loss: 1.6016812811476981\n",
      "Epoch: 77 completed in: 6.2195494174957275 s. Training loss: 0.019103803780664057 . Val loss: 0.02375642182305455 . Test loss: 1.6114189008764217\n",
      "Epoch: 78 completed in: 6.278828382492065 s. Training loss: 0.019080083504536726 . Val loss: 0.02373812352307141 . Test loss: 1.6096795291313424\n",
      "Epoch: 79 completed in: 6.385275840759277 s. Training loss: 0.019038847878738475 . Val loss: 0.023721927683800458 . Test loss: 1.6038474426467417\n",
      "Epoch: 80 completed in: 6.176747798919678 s. Training loss: 0.019012788031930034 . Val loss: 0.02370641431771219 . Test loss: 1.6060222333307503\n",
      "8.257687192506786e-05\n",
      "Epoch: 81 completed in: 6.169775485992432 s. Training loss: 0.018977041900103544 . Val loss: 0.023674285924062132 . Test loss: 1.6078752496893385\n",
      "Epoch: 82 completed in: 6.158658266067505 s. Training loss: 0.018953091145431908 . Val loss: 0.023673259280622007 . Test loss: 1.6102409844117311\n",
      "Epoch: 83 completed in: 6.3490495681762695 s. Training loss: 0.01890934120020727 . Val loss: 0.023643254395574333 . Test loss: 1.6133893248069293\n",
      "Epoch: 84 completed in: 6.166191339492798 s. Training loss: 0.018879063617791573 . Val loss: 0.023627076344564558 . Test loss: 1.612042965504278\n",
      "Epoch: 85 completed in: 6.252426862716675 s. Training loss: 0.018874348549131493 . Val loss: 0.02361537949182093 . Test loss: 1.6068008316565383\n",
      "6.389640937399643e-05\n",
      "Epoch: 86 completed in: 6.1504065990448 s. Training loss: 0.0188320121680298 . Val loss: 0.023591991467401385 . Test loss: 1.609270503795728\n",
      "Epoch: 87 completed in: 6.322261571884155 s. Training loss: 0.01879904230200026 . Val loss: 0.023587461188435556 . Test loss: 1.6123080358393187\n",
      "Epoch: 88 completed in: 6.248615503311157 s. Training loss: 0.018789689233322648 . Val loss: 0.023577516619116067 . Test loss: 1.6109818227068236\n",
      "Epoch: 89 completed in: 6.091031312942505 s. Training loss: 0.018775801922119882 . Val loss: 0.023575895512476564 . Test loss: 1.6122669966489866\n",
      "Epoch: 90 completed in: 6.120739698410034 s. Training loss: 0.01874794859275983 . Val loss: 0.023556073755025865 . Test loss: 1.6124418732772374\n",
      "4.944182354829473e-05\n",
      "Epoch: 91 completed in: 6.270354509353638 s. Training loss: 0.01873327660489909 . Val loss: 0.023540535708889365 . Test loss: 1.613210598702586\n",
      "Epoch: 92 completed in: 6.330728054046631 s. Training loss: 0.018702048536417257 . Val loss: 0.02353671849705279 . Test loss: 1.6128908733827072\n",
      "Epoch: 93 completed in: 6.2266058921813965 s. Training loss: 0.018702497515473922 . Val loss: 0.02352375234477222 . Test loss: 1.613278914117502\n",
      "Epoch: 94 completed in: 6.03107762336731 s. Training loss: 0.01867442136888739 . Val loss: 0.023512397706508637 . Test loss: 1.6147683682600587\n",
      "Epoch: 95 completed in: 6.199386835098267 s. Training loss: 0.018686065300755256 . Val loss: 0.023494215868413448 . Test loss: 1.6147819797942857\n",
      "3.825714057690906e-05\n",
      "Epoch: 96 completed in: 6.200141429901123 s. Training loss: 0.018656285364099228 . Val loss: 0.023495031893253325 . Test loss: 1.6130219910419517\n",
      "Epoch: 97 completed in: 6.284588575363159 s. Training loss: 0.018621122804436372 . Val loss: 0.02348954170010984 . Test loss: 1.616662035384986\n",
      "Epoch: 98 completed in: 6.402582883834839 s. Training loss: 0.018605369350526237 . Val loss: 0.023476288793608545 . Test loss: 1.6155717340265139\n",
      "Epoch: 99 completed in: 6.326343774795532 s. Training loss: 0.01861726764562356 . Val loss: 0.023476316360756755 . Test loss: 1.6174324349482831\n",
      "Epoch: 100 completed in: 6.3421642780303955 s. Training loss: 0.01860877849759847 . Val loss: 0.023466893844306468 . Test loss: 1.615431259894822\n",
      "2.9602646101669974e-05\n",
      "1.615431259894822\n",
      "2381884\n",
      "------Trial 2\n",
      "Epoch: 1 completed in: 6.328340768814087 s. Training loss: 0.17069939562003977 . Val loss: 0.11527420282363891 . Test loss: 1.2199853646310101\n",
      "Epoch: 2 completed in: 6.0923912525177 s. Training loss: 0.09736748216469793 . Val loss: 0.09533750545233488 . Test loss: 1.2500344135451866\n",
      "Epoch: 3 completed in: 6.172400951385498 s. Training loss: 0.08820988944847219 . Val loss: 0.09092226084321738 . Test loss: 1.2683708761143757\n",
      "Epoch: 4 completed in: 6.063091039657593 s. Training loss: 0.0811995125654405 . Val loss: 0.08016067463904619 . Test loss: 1.2542964979937368\n",
      "Epoch: 5 completed in: 6.101593494415283 s. Training loss: 0.0752692798927535 . Val loss: 0.07929627280682325 . Test loss: 1.3645700215210705\n",
      "0.0038689046874999995\n",
      "Epoch: 6 completed in: 6.202796936035156 s. Training loss: 0.0709279138785209 . Val loss: 0.07584185795858503 . Test loss: 1.310926760919606\n",
      "Epoch: 7 completed in: 6.367531061172485 s. Training loss: 0.06720136993829787 . Val loss: 0.06703980201855302 . Test loss: 1.3058407530500058\n",
      "Epoch: 8 completed in: 6.376962184906006 s. Training loss: 0.0629037442161654 . Val loss: 0.0645624672062695 . Test loss: 1.304227795688808\n",
      "Epoch: 9 completed in: 6.400618553161621 s. Training loss: 0.05927433183647855 . Val loss: 0.06272102054208517 . Test loss: 1.3147198116030447\n",
      "Epoch: 10 completed in: 6.426060676574707 s. Training loss: 0.05685669682702444 . Val loss: 0.05995904318988323 . Test loss: 1.3534396045174917\n",
      "0.002993684696191893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 completed in: 6.220562696456909 s. Training loss: 0.05305225897009355 . Val loss: 0.05655083348974586 . Test loss: 1.3509532669434954\n",
      "Epoch: 12 completed in: 6.158578872680664 s. Training loss: 0.05054320853856141 . Val loss: 0.05216180020943284 . Test loss: 1.4022745013390547\n",
      "Epoch: 13 completed in: 5.956266641616821 s. Training loss: 0.04685705176887721 . Val loss: 0.050427628494799134 . Test loss: 1.3739797890372418\n",
      "Epoch: 14 completed in: 6.14324951171875 s. Training loss: 0.04491558946995405 . Val loss: 0.048991707153618336 . Test loss: 1.3688670777897702\n",
      "Epoch: 15 completed in: 6.0730881690979 s. Training loss: 0.04303452968053574 . Val loss: 0.049405740760266784 . Test loss: 1.4053218545269808\n",
      "0.002316456150798765\n",
      "Epoch: 16 completed in: 6.15858006477356 s. Training loss: 0.04194206369184229 . Val loss: 0.04496374754235148 . Test loss: 1.3833124147421336\n",
      "Epoch: 17 completed in: 5.920493125915527 s. Training loss: 0.03947826141804239 . Val loss: 0.04312934586778283 . Test loss: 1.4055381658603674\n",
      "Epoch: 18 completed in: 6.3113110065460205 s. Training loss: 0.03780617927248678 . Val loss: 0.04191993456333876 . Test loss: 1.4057979622162373\n",
      "Epoch: 19 completed in: 6.14615535736084 s. Training loss: 0.036052538965740344 . Val loss: 0.039665494859218595 . Test loss: 1.396253362215675\n",
      "Epoch: 20 completed in: 6.285562038421631 s. Training loss: 0.03503833047664948 . Val loss: 0.03853519381955266 . Test loss: 1.4194449874933295\n",
      "0.0017924296120427094\n",
      "Epoch: 21 completed in: 6.04646372795105 s. Training loss: 0.03415939447735131 . Val loss: 0.03773475363850594 . Test loss: 1.419287220404911\n",
      "Epoch: 22 completed in: 6.1743927001953125 s. Training loss: 0.03267846315636905 . Val loss: 0.03639533352106809 . Test loss: 1.4299172582349422\n",
      "Epoch: 23 completed in: 6.068820238113403 s. Training loss: 0.031800391274864656 . Val loss: 0.03573655281215906 . Test loss: 1.4223174794511464\n",
      "Epoch: 24 completed in: 6.461016893386841 s. Training loss: 0.030992772654514677 . Val loss: 0.03476952291093767 . Test loss: 1.4291996836605294\n",
      "Epoch: 25 completed in: 6.223172426223755 s. Training loss: 0.03051934003095775 . Val loss: 0.034475763142108914 . Test loss: 1.4378198189894145\n",
      "0.0013869478656091687\n",
      "Epoch: 26 completed in: 6.27322244644165 s. Training loss: 0.02951083828552361 . Val loss: 0.033439669944345954 . Test loss: 1.4310983187683501\n",
      "Epoch: 27 completed in: 6.421553611755371 s. Training loss: 0.028942694132943658 . Val loss: 0.032821099832654 . Test loss: 1.4527938843479744\n",
      "Epoch: 28 completed in: 6.589879274368286 s. Training loss: 0.028826351447479567 . Val loss: 0.03282475695014 . Test loss: 1.4343706168511348\n",
      "Epoch: 29 completed in: 6.0525453090667725 s. Training loss: 0.028038154444555298 . Val loss: 0.03160522421821952 . Test loss: 1.4374011577857897\n",
      "Epoch: 30 completed in: 6.271771430969238 s. Training loss: 0.0271877714969816 . Val loss: 0.03092526695691049 . Test loss: 1.449664267831732\n",
      "0.0010731938197146862\n",
      "Epoch: 31 completed in: 6.204362869262695 s. Training loss: 0.026570387538114602 . Val loss: 0.030608902033418418 . Test loss: 1.4411513952066222\n",
      "Epoch: 32 completed in: 6.088266611099243 s. Training loss: 0.026171163118777485 . Val loss: 0.030309729231521487 . Test loss: 1.4819325647180723\n",
      "Epoch: 33 completed in: 6.049762487411499 s. Training loss: 0.025884027354908686 . Val loss: 0.029951146570965648 . Test loss: 1.475119018207593\n",
      "Epoch: 34 completed in: 6.123526334762573 s. Training loss: 0.02540915535531775 . Val loss: 0.0295011832844466 . Test loss: 1.4544753404941144\n",
      "Epoch: 35 completed in: 6.271653413772583 s. Training loss: 0.02501113340938396 . Val loss: 0.02920164163224399 . Test loss: 1.4601878771318255\n",
      "0.0008304169199380356\n",
      "Epoch: 36 completed in: 6.087225914001465 s. Training loss: 0.024702028935625606 . Val loss: 0.029064605105668308 . Test loss: 1.4741734591625142\n",
      "Epoch: 37 completed in: 6.101095199584961 s. Training loss: 0.02468204708348443 . Val loss: 0.028623446682468058 . Test loss: 1.4871522414599785\n",
      "Epoch: 38 completed in: 5.983201503753662 s. Training loss: 0.02438157115022849 . Val loss: 0.028196087712422012 . Test loss: 1.4663154481514955\n",
      "Epoch: 39 completed in: 6.293584108352661 s. Training loss: 0.02410674269861766 . Val loss: 0.02792770033702254 . Test loss: 1.4738624363275818\n",
      "Epoch: 40 completed in: 6.063105344772339 s. Training loss: 0.023461132935744568 . Val loss: 0.027569917775690556 . Test loss: 1.476054318085991\n",
      "0.0006425607828255154\n",
      "Epoch: 41 completed in: 6.0580644607543945 s. Training loss: 0.023300891531373026 . Val loss: 0.02744271019473672 . Test loss: 1.4772178357627188\n",
      "Epoch: 42 completed in: 5.950474977493286 s. Training loss: 0.023015579133953926 . Val loss: 0.027192693622782826 . Test loss: 1.4754964066485823\n",
      "Epoch: 43 completed in: 6.315564393997192 s. Training loss: 0.02273547448163485 . Val loss: 0.027009874396026135 . Test loss: 1.4802884170934054\n",
      "Epoch: 44 completed in: 6.068570137023926 s. Training loss: 0.022524373959342057 . Val loss: 0.026744038611650468 . Test loss: 1.4934116327345617\n",
      "Epoch: 45 completed in: 6.214961767196655 s. Training loss: 0.022421087525839354 . Val loss: 0.026830302877351643 . Test loss: 1.4883475803907649\n",
      "0.000497201284935461\n",
      "Epoch: 46 completed in: 6.2813990116119385 s. Training loss: 0.022229621322811956 . Val loss: 0.026416951278224587 . Test loss: 1.4969264948347856\n",
      "Epoch: 47 completed in: 6.192653179168701 s. Training loss: 0.02189446326997811 . Val loss: 0.026199356047436596 . Test loss: 1.4972932997324007\n",
      "Epoch: 48 completed in: 6.000693321228027 s. Training loss: 0.02172371957206378 . Val loss: 0.026122235413640737 . Test loss: 1.493235015347641\n",
      "Epoch: 49 completed in: 6.228606939315796 s. Training loss: 0.021630927070594618 . Val loss: 0.026009717490524054 . Test loss: 1.5050967504023947\n",
      "Epoch: 50 completed in: 5.940802097320557 s. Training loss: 0.0214633061992426 . Val loss: 0.02588057271204889 . Test loss: 1.5081054291424454\n",
      "0.0003847248763835656\n",
      "Epoch: 51 completed in: 6.144866466522217 s. Training loss: 0.02136913313102113 . Val loss: 0.025740905478596688 . Test loss: 1.5086112913907856\n",
      "Epoch: 52 completed in: 5.964083194732666 s. Training loss: 0.021138664542350673 . Val loss: 0.02553686765022576 . Test loss: 1.5063521247120935\n",
      "Epoch: 53 completed in: 6.030091047286987 s. Training loss: 0.021035773801977617 . Val loss: 0.02543828566558659 . Test loss: 1.5098991082648352\n",
      "Epoch: 54 completed in: 5.924365997314453 s. Training loss: 0.020917333465368643 . Val loss: 0.02534871636889875 . Test loss: 1.5157393805144865\n",
      "Epoch: 55 completed in: 6.06342077255249 s. Training loss: 0.02079145886330274 . Val loss: 0.02519176248461008 . Test loss: 1.5201239113864855\n",
      "0.000297692775527647\n",
      "Epoch: 56 completed in: 6.111428260803223 s. Training loss: 0.02068111414674425 . Val loss: 0.025127689260989426 . Test loss: 1.5204127242219116\n",
      "Epoch: 57 completed in: 6.01187801361084 s. Training loss: 0.020567572376534452 . Val loss: 0.02507322165183723 . Test loss: 1.5134873730887262\n",
      "Epoch: 58 completed in: 6.1737751960754395 s. Training loss: 0.0205083570381912 . Val loss: 0.024997520726174116 . Test loss: 1.5184077467330708\n",
      "Epoch: 59 completed in: 6.05312442779541 s. Training loss: 0.02044551867148737 . Val loss: 0.02491883500479162 . Test loss: 1.5275338510945176\n",
      "Epoch: 60 completed in: 6.229980230331421 s. Training loss: 0.02027919566272384 . Val loss: 0.024834101228043436 . Test loss: 1.5242317513110193\n",
      "0.00023034899493475967\n",
      "Epoch: 61 completed in: 5.998678684234619 s. Training loss: 0.020195591943270533 . Val loss: 0.024745169514790177 . Test loss: 1.5148712040297294\n",
      "Epoch: 62 completed in: 6.099148750305176 s. Training loss: 0.020113415356698264 . Val loss: 0.024693409120664 . Test loss: 1.5273431163540763\n",
      "Epoch: 63 completed in: 6.074662208557129 s. Training loss: 0.02006038076441436 . Val loss: 0.024665674194693565 . Test loss: 1.5288516198153337\n",
      "Epoch: 64 completed in: 6.316465377807617 s. Training loss: 0.019993399677757363 . Val loss: 0.024597726855427028 . Test loss: 1.5284115264615765\n",
      "Epoch: 65 completed in: 6.184937953948975 s. Training loss: 0.019897526803079747 . Val loss: 0.024559813644737007 . Test loss: 1.5245847817118454\n",
      "0.00017823966125280104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66 completed in: 6.432301044464111 s. Training loss: 0.01984001038065792 . Val loss: 0.02448876635171473 . Test loss: 1.523813164101202\n",
      "Epoch: 67 completed in: 6.271291017532349 s. Training loss: 0.01978000273153077 . Val loss: 0.024478418147191407 . Test loss: 1.5280794434810645\n",
      "Epoch: 68 completed in: 6.427848815917969 s. Training loss: 0.019706854725895572 . Val loss: 0.02439511208795011 . Test loss: 1.5230340424793278\n",
      "Epoch: 69 completed in: 5.974054574966431 s. Training loss: 0.019664444477997558 . Val loss: 0.024357642186805605 . Test loss: 1.523735571592192\n",
      "Epoch: 70 completed in: 6.131288528442383 s. Training loss: 0.019596461363028 . Val loss: 0.02431506202556193 . Test loss: 1.5282369867008962\n",
      "0.00013791845218387477\n",
      "Epoch: 71 completed in: 5.929095983505249 s. Training loss: 0.019548439318790054 . Val loss: 0.02428579544648528 . Test loss: 1.5249706003327879\n",
      "Epoch: 72 completed in: 6.3479602336883545 s. Training loss: 0.01951157058976645 . Val loss: 0.02424650136381388 . Test loss: 1.523852145273128\n",
      "Epoch: 73 completed in: 6.058180093765259 s. Training loss: 0.019443220017056395 . Val loss: 0.024236481031402947 . Test loss: 1.5259187856116823\n",
      "Epoch: 74 completed in: 6.1879355907440186 s. Training loss: 0.01940866170219914 . Val loss: 0.024209033604711293 . Test loss: 1.5229527601435804\n",
      "Epoch: 75 completed in: 6.515557289123535 s. Training loss: 0.019378833710657854 . Val loss: 0.024157572723925113 . Test loss: 1.5298296592329121\n",
      "0.00010671866922938751\n",
      "Epoch: 76 completed in: 6.278496742248535 s. Training loss: 0.019302387194993505 . Val loss: 0.024156802613288165 . Test loss: 1.5314988386841923\n",
      "Epoch: 77 completed in: 6.2617926597595215 s. Training loss: 0.019280269986739124 . Val loss: 0.024120323127135634 . Test loss: 1.5258295224988598\n",
      "Epoch: 78 completed in: 6.297502756118774 s. Training loss: 0.01923850454716352 . Val loss: 0.024094492988660933 . Test loss: 1.5300909911007476\n",
      "Epoch: 79 completed in: 6.17716908454895 s. Training loss: 0.01921881564695687 . Val loss: 0.02407492776401341 . Test loss: 1.5325318716631477\n",
      "Epoch: 80 completed in: 5.901494026184082 s. Training loss: 0.019214533622900064 . Val loss: 0.02403895854949951 . Test loss: 1.531564749454025\n",
      "8.257687192506786e-05\n",
      "Epoch: 81 completed in: 7.194155931472778 s. Training loss: 0.019133981313882737 . Val loss: 0.02401537839323282 . Test loss: 1.5301081032266417\n",
      "Epoch: 82 completed in: 6.580045938491821 s. Training loss: 0.019126548703732718 . Val loss: 0.024017214123159648 . Test loss: 1.5289906826793356\n",
      "Epoch: 83 completed in: 6.409699440002441 s. Training loss: 0.019094618961867624 . Val loss: 0.02398000219836831 . Test loss: 1.5285248953737645\n",
      "Epoch: 84 completed in: 6.2790207862854 s. Training loss: 0.01907010721110732 . Val loss: 0.023976685106754304 . Test loss: 1.5315250245205814\n",
      "Epoch: 85 completed in: 6.046658992767334 s. Training loss: 0.019022598009502147 . Val loss: 0.023948133736848832 . Test loss: 1.5283914094211677\n",
      "6.389640937399643e-05\n",
      "Epoch: 86 completed in: 5.941497802734375 s. Training loss: 0.01901066975572901 . Val loss: 0.023935057362541555 . Test loss: 1.530598200925935\n",
      "Epoch: 87 completed in: 6.53753662109375 s. Training loss: 0.018983248396892183 . Val loss: 0.023922586953267454 . Test loss: 1.5325524437980291\n",
      "Epoch: 88 completed in: 6.341167449951172 s. Training loss: 0.01895752922380275 . Val loss: 0.023932654224336148 . Test loss: 1.5326319801674926\n",
      "Epoch: 89 completed in: 6.061598300933838 s. Training loss: 0.01893070723562345 . Val loss: 0.02390649262815714 . Test loss: 1.5307685332843002\n",
      "Epoch: 90 completed in: 6.092061996459961 s. Training loss: 0.018914901834987376 . Val loss: 0.023887025751173498 . Test loss: 1.5316942494086454\n",
      "4.944182354829473e-05\n",
      "Epoch: 91 completed in: 6.339239597320557 s. Training loss: 0.018889230791560924 . Val loss: 0.023866082448512314 . Test loss: 1.5325666040855483\n",
      "Epoch: 92 completed in: 6.297936916351318 s. Training loss: 0.018928481979689896 . Val loss: 0.023867045855149628 . Test loss: 1.5328098459398096\n",
      "Epoch: 93 completed in: 6.319340705871582 s. Training loss: 0.01885765404814351 . Val loss: 0.02386333830654621 . Test loss: 1.5308536719152228\n",
      "Epoch: 94 completed in: 6.087583541870117 s. Training loss: 0.01884645770854541 . Val loss: 0.02385228327475488 . Test loss: 1.5314448202573034\n",
      "Epoch: 95 completed in: 6.4557719230651855 s. Training loss: 0.018858500266869138 . Val loss: 0.023834500974044204 . Test loss: 1.5329401237157196\n",
      "3.825714057690906e-05\n",
      "Epoch: 96 completed in: 6.024582862854004 s. Training loss: 0.01883075442029177 . Val loss: 0.02384227020666003 . Test loss: 1.5347531510385846\n",
      "Epoch: 97 completed in: 6.322598695755005 s. Training loss: 0.018799978192814076 . Val loss: 0.02382109882310033 . Test loss: 1.5325234274768904\n",
      "Epoch: 98 completed in: 5.99465799331665 s. Training loss: 0.018798961079794996 . Val loss: 0.02381377969868481 . Test loss: 1.5331322165033825\n",
      "Epoch: 99 completed in: 6.123589277267456 s. Training loss: 0.018776852746296972 . Val loss: 0.02381048216484487 . Test loss: 1.5321040462297846\n",
      "Epoch: 100 completed in: 6.010641813278198 s. Training loss: 0.018753630370853373 . Val loss: 0.02380584324710071 . Test loss: 1.5319617172576125\n",
      "2.9602646101669974e-05\n",
      "1.5319617172576125\n",
      "2381884\n",
      "------Trial 3\n",
      "Epoch: 1 completed in: 6.255228042602539 s. Training loss: 0.1689520941870491 . Val loss: 0.12846306357532739 . Test loss: 1.3302419832688082\n",
      "Epoch: 2 completed in: 6.364558696746826 s. Training loss: 0.09826981953352038 . Val loss: 0.0921737290918827 . Test loss: 1.188718474138989\n",
      "Epoch: 3 completed in: 6.2993738651275635 s. Training loss: 0.08628148028123987 . Val loss: 0.08470808155834675 . Test loss: 1.2315562947480914\n",
      "Epoch: 4 completed in: 6.091606855392456 s. Training loss: 0.08005665588009096 . Val loss: 0.08368575144559146 . Test loss: 1.2602091010138596\n",
      "Epoch: 5 completed in: 6.3238842487335205 s. Training loss: 0.0749990161725857 . Val loss: 0.07581197516992688 . Test loss: 1.2588164525429228\n",
      "0.0038689046874999995\n",
      "Epoch: 6 completed in: 6.2112884521484375 s. Training loss: 0.06907306360012859 . Val loss: 0.07396085113286972 . Test loss: 1.3520440035375927\n",
      "Epoch: 7 completed in: 6.485121011734009 s. Training loss: 0.06615393398071293 . Val loss: 0.06785793667659164 . Test loss: 1.2964474635910117\n",
      "Epoch: 8 completed in: 5.941860198974609 s. Training loss: 0.06088428061972134 . Val loss: 0.06348852422088384 . Test loss: 1.328243058511225\n",
      "Epoch: 9 completed in: 6.320812940597534 s. Training loss: 0.05732074033224235 . Val loss: 0.06000631554052234 . Test loss: 1.4214161392958409\n",
      "Epoch: 10 completed in: 6.177896499633789 s. Training loss: 0.054771092115309984 . Val loss: 0.05993506433442235 . Test loss: 1.3697097600325783\n",
      "0.002993684696191893\n",
      "Epoch: 11 completed in: 6.336711883544922 s. Training loss: 0.051970870308849934 . Val loss: 0.05508680064231157 . Test loss: 1.4203660627632197\n",
      "Epoch: 12 completed in: 6.168344020843506 s. Training loss: 0.048275175878275046 . Val loss: 0.052390841953456405 . Test loss: 1.426256927500714\n",
      "Epoch: 13 completed in: 6.487624168395996 s. Training loss: 0.0456699197500074 . Val loss: 0.049091812875121835 . Test loss: 1.3833457050649793\n",
      "Epoch: 14 completed in: 6.215193033218384 s. Training loss: 0.04369584097098695 . Val loss: 0.0474200245924294 . Test loss: 1.3654608020479757\n",
      "Epoch: 15 completed in: 6.354867935180664 s. Training loss: 0.04122029132053365 . Val loss: 0.044173174258321526 . Test loss: 1.3575431428735316\n",
      "0.002316456150798765\n",
      "Epoch: 16 completed in: 6.171916484832764 s. Training loss: 0.03942854002281262 . Val loss: 0.043624653853476046 . Test loss: 1.4389261258918138\n",
      "Epoch: 17 completed in: 6.0832359790802 s. Training loss: 0.03779337252660172 . Val loss: 0.04222084460780025 . Test loss: 1.4853591024943653\n",
      "Epoch: 18 completed in: 6.178806781768799 s. Training loss: 0.03633335832316075 . Val loss: 0.03957613594830036 . Test loss: 1.4292656988741932\n",
      "Epoch: 19 completed in: 6.195832967758179 s. Training loss: 0.034879335807296484 . Val loss: 0.03817439740523696 . Test loss: 1.4445673857805743\n",
      "Epoch: 20 completed in: 6.084040880203247 s. Training loss: 0.03378865462693855 . Val loss: 0.038958041183650496 . Test loss: 1.4262088141511255\n",
      "0.0017924296120427094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 completed in: 6.166500091552734 s. Training loss: 0.03385437351318389 . Val loss: 0.03728067297488451 . Test loss: 1.4553449105883465\n",
      "Epoch: 22 completed in: 6.3365638256073 s. Training loss: 0.03216346386602543 . Val loss: 0.03651466984301806 . Test loss: 1.491271503436807\n",
      "Epoch: 23 completed in: 6.363039016723633 s. Training loss: 0.03183081421837972 . Val loss: 0.034561747033149 . Test loss: 1.5221160956283908\n",
      "Epoch: 24 completed in: 6.415479898452759 s. Training loss: 0.030561824281611583 . Val loss: 0.034387984732165935 . Test loss: 1.4925640492754841\n",
      "Epoch: 25 completed in: 6.056412220001221 s. Training loss: 0.029639524577634177 . Val loss: 0.03376252502202988 . Test loss: 1.574431417921438\n",
      "0.0013869478656091687\n",
      "Epoch: 26 completed in: 6.275840759277344 s. Training loss: 0.028830945763709773 . Val loss: 0.03260737960226834 . Test loss: 1.5334044520819174\n",
      "Epoch: 27 completed in: 6.480907678604126 s. Training loss: 0.028228789207654714 . Val loss: 0.032001563580706716 . Test loss: 1.5263166514023985\n",
      "Epoch: 28 completed in: 6.550425291061401 s. Training loss: 0.027990381498515172 . Val loss: 0.031414565583691004 . Test loss: 1.5355562033669528\n",
      "Epoch: 29 completed in: 6.3550615310668945 s. Training loss: 0.027204581767483783 . Val loss: 0.0313107751775533 . Test loss: 1.5430348462738228\n",
      "Epoch: 30 completed in: 6.400909185409546 s. Training loss: 0.0266524377979175 . Val loss: 0.031023306446149944 . Test loss: 1.569815538381384\n",
      "0.0010731938197146862\n",
      "Epoch: 31 completed in: 6.109249114990234 s. Training loss: 0.026272822558934237 . Val loss: 0.02990408749319613 . Test loss: 1.5226290221998762\n",
      "Epoch: 32 completed in: 5.984121561050415 s. Training loss: 0.02566209164903547 . Val loss: 0.029781769821420313 . Test loss: 1.5411412567917653\n",
      "Epoch: 33 completed in: 5.88146448135376 s. Training loss: 0.025549967365380188 . Val loss: 0.02992520681582391 . Test loss: 1.5635904255184856\n",
      "Epoch: 34 completed in: 6.023168563842773 s. Training loss: 0.025026188388358066 . Val loss: 0.028921425668522717 . Test loss: 1.5460872581993863\n",
      "Epoch: 35 completed in: 5.806783437728882 s. Training loss: 0.024654003308854835 . Val loss: 0.02843089671805501 . Test loss: 1.5793565183187126\n",
      "0.0008304169199380356\n",
      "Epoch: 36 completed in: 5.982808828353882 s. Training loss: 0.02419108021188609 . Val loss: 0.028134558303281664 . Test loss: 1.5476889862686563\n",
      "Epoch: 37 completed in: 5.916761159896851 s. Training loss: 0.023818679483621007 . Val loss: 0.027969768038019538 . Test loss: 1.5845098259062191\n",
      "Epoch: 38 completed in: 5.9244420528411865 s. Training loss: 0.02357037951421999 . Val loss: 0.027602832205593585 . Test loss: 1.5749875796676036\n",
      "Epoch: 39 completed in: 5.799323320388794 s. Training loss: 0.023356042115738357 . Val loss: 0.027612050995230675 . Test loss: 1.594899147627822\n",
      "Epoch: 40 completed in: 5.813990592956543 s. Training loss: 0.02314773415398859 . Val loss: 0.027162339398637415 . Test loss: 1.5808979045214073\n",
      "0.0006425607828255154\n",
      "Epoch: 41 completed in: 5.870970726013184 s. Training loss: 0.022738642936205342 . Val loss: 0.026858905097469687 . Test loss: 1.5929815237902918\n",
      "Epoch: 42 completed in: 5.807375431060791 s. Training loss: 0.02248438211144322 . Val loss: 0.026826177863404153 . Test loss: 1.6129748993591744\n",
      "Epoch: 43 completed in: 5.947608947753906 s. Training loss: 0.02239228554586642 . Val loss: 0.026525287376716732 . Test loss: 1.6111463716504748\n",
      "Epoch: 44 completed in: 5.844454765319824 s. Training loss: 0.022106994772805786 . Val loss: 0.026357169775292278 . Test loss: 1.6189477386131215\n",
      "Epoch: 45 completed in: 5.968111515045166 s. Training loss: 0.021934516904671696 . Val loss: 0.026149304257705806 . Test loss: 1.606060495934534\n",
      "0.000497201284935461\n",
      "Epoch: 46 completed in: 5.851786851882935 s. Training loss: 0.021710911126684967 . Val loss: 0.025946329487487672 . Test loss: 1.6175049217767195\n",
      "Epoch: 47 completed in: 5.997452735900879 s. Training loss: 0.021517453293730743 . Val loss: 0.025845854450017212 . Test loss: 1.612505203624509\n",
      "Epoch: 48 completed in: 5.859036684036255 s. Training loss: 0.021399858373686347 . Val loss: 0.02576736519113183 . Test loss: 1.6103505473621293\n",
      "Epoch: 49 completed in: 5.969377756118774 s. Training loss: 0.02128253524348031 . Val loss: 0.025573650002479555 . Test loss: 1.6105264936999846\n",
      "Epoch: 50 completed in: 5.877536296844482 s. Training loss: 0.02107469804829707 . Val loss: 0.02544079627841711 . Test loss: 1.6207951727399148\n",
      "0.0003847248763835656\n",
      "Epoch: 51 completed in: 5.924025297164917 s. Training loss: 0.020916727283140167 . Val loss: 0.025366276688873767 . Test loss: 1.6256711430189552\n",
      "Epoch: 52 completed in: 6.5507588386535645 s. Training loss: 0.020821162791800324 . Val loss: 0.025214558653533458 . Test loss: 1.6102170886139258\n",
      "Epoch: 53 completed in: 6.156765460968018 s. Training loss: 0.020720178927600818 . Val loss: 0.02512861918658018 . Test loss: 1.6149334261064152\n",
      "Epoch: 54 completed in: 6.422775030136108 s. Training loss: 0.020568414795192055 . Val loss: 0.025061400746926664 . Test loss: 1.6224847996032805\n",
      "Epoch: 55 completed in: 6.115131616592407 s. Training loss: 0.020425781002608095 . Val loss: 0.024913220899179578 . Test loss: 1.6254546080633308\n",
      "0.000297692775527647\n",
      "Epoch: 56 completed in: 6.220545530319214 s. Training loss: 0.020344848941712484 . Val loss: 0.024867342552170157 . Test loss: 1.6285464874990316\n",
      "Epoch: 57 completed in: 6.018915176391602 s. Training loss: 0.020232219024676912 . Val loss: 0.024827109230682255 . Test loss: 1.6254384941728324\n",
      "Epoch: 58 completed in: 6.28102707862854 s. Training loss: 0.020156293957881682 . Val loss: 0.02475812379270792 . Test loss: 1.61785511608868\n",
      "Epoch: 59 completed in: 5.900933027267456 s. Training loss: 0.020046513670389236 . Val loss: 0.02465770309790969 . Test loss: 1.6159485606535249\n",
      "Epoch: 60 completed in: 6.2708728313446045 s. Training loss: 0.01999804978496837 . Val loss: 0.024566799867898226 . Test loss: 1.6209491682332622\n",
      "0.00023034899493475967\n",
      "Epoch: 61 completed in: 6.031012296676636 s. Training loss: 0.01987710734489408 . Val loss: 0.024544426891952752 . Test loss: 1.6281477043769528\n",
      "Epoch: 62 completed in: 6.3144755363464355 s. Training loss: 0.019811528408560004 . Val loss: 0.024482696317136288 . Test loss: 1.6205581410525212\n",
      "Epoch: 63 completed in: 5.865418910980225 s. Training loss: 0.0197398477050401 . Val loss: 0.024442094983533023 . Test loss: 1.6206190195681354\n",
      "Epoch: 64 completed in: 6.2693305015563965 s. Training loss: 0.019661668416139852 . Val loss: 0.024373017624020575 . Test loss: 1.6275503309519788\n",
      "Epoch: 65 completed in: 6.331486225128174 s. Training loss: 0.019593925215303898 . Val loss: 0.02434389516711235 . Test loss: 1.6267574674806398\n",
      "0.00017823966125280104\n",
      "Epoch: 66 completed in: 6.291417360305786 s. Training loss: 0.019504289839579893 . Val loss: 0.024287075735628605 . Test loss: 1.6265591455286017\n",
      "Epoch: 67 completed in: 6.148988723754883 s. Training loss: 0.019477906242611198 . Val loss: 0.02424815590493381 . Test loss: 1.6188995506160662\n",
      "Epoch: 68 completed in: 6.089104890823364 s. Training loss: 0.019406335197225975 . Val loss: 0.024199617886915804 . Test loss: 1.6239263699915123\n",
      "Epoch: 69 completed in: 6.17820143699646 s. Training loss: 0.019361184066990868 . Val loss: 0.024148680595681073 . Test loss: 1.6302197720892262\n",
      "Epoch: 70 completed in: 6.405059337615967 s. Training loss: 0.019328421933481294 . Val loss: 0.024133486766368152 . Test loss: 1.6262782117108319\n",
      "0.00013791845218387477\n",
      "Epoch: 71 completed in: 6.391512393951416 s. Training loss: 0.01926793279058307 . Val loss: 0.0240971886087209 . Test loss: 1.6221757473888936\n",
      "Epoch: 72 completed in: 6.071774482727051 s. Training loss: 0.01921730269643947 . Val loss: 0.02403468210250139 . Test loss: 1.6241923595405867\n",
      "Epoch: 73 completed in: 5.868118762969971 s. Training loss: 0.019173584069485647 . Val loss: 0.024041995173320174 . Test loss: 1.6217833627408198\n",
      "Epoch: 74 completed in: 5.92367696762085 s. Training loss: 0.01911927867735172 . Val loss: 0.02402449338696897 . Test loss: 1.6256384088929765\n",
      "Epoch: 75 completed in: 5.8686583042144775 s. Training loss: 0.019087209549807284 . Val loss: 0.02396834478713572 . Test loss: 1.6277214804813906\n",
      "0.00010671866922938751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76 completed in: 5.806458950042725 s. Training loss: 0.019047942637962147 . Val loss: 0.023954405961558223 . Test loss: 1.6241906186991426\n",
      "Epoch: 77 completed in: 5.883332967758179 s. Training loss: 0.01900004045561935 . Val loss: 0.023920277087017893 . Test loss: 1.6248344007763236\n",
      "Epoch: 78 completed in: 5.767704010009766 s. Training loss: 0.01896168766067411 . Val loss: 0.023913544369861484 . Test loss: 1.6282113426387577\n",
      "Epoch: 79 completed in: 5.9882237911224365 s. Training loss: 0.018929479895227582 . Val loss: 0.023879720456898214 . Test loss: 1.629141086518781\n",
      "Epoch: 80 completed in: 5.824932813644409 s. Training loss: 0.018897563667736785 . Val loss: 0.02386851804330945 . Test loss: 1.6254702167636466\n",
      "8.257687192506786e-05\n",
      "Epoch: 81 completed in: 5.960302829742432 s. Training loss: 0.0188955346421495 . Val loss: 0.023857502825558187 . Test loss: 1.6277960420551458\n",
      "Epoch: 82 completed in: 5.8171164989471436 s. Training loss: 0.018859960838989186 . Val loss: 0.02383606219664216 . Test loss: 1.6282781054349145\n",
      "Epoch: 83 completed in: 5.936996936798096 s. Training loss: 0.018807138071606194 . Val loss: 0.023812099918723108 . Test loss: 1.6293864361044645\n",
      "Epoch: 84 completed in: 5.801955699920654 s. Training loss: 0.01880684433790454 . Val loss: 0.023806147277355194 . Test loss: 1.6243659522602554\n",
      "Epoch: 85 completed in: 5.955773115158081 s. Training loss: 0.018766831586232584 . Val loss: 0.02379920887760818 . Test loss: 1.6242628870177924\n",
      "6.389640937399643e-05\n",
      "Epoch: 86 completed in: 5.854746103286743 s. Training loss: 0.018731660970969358 . Val loss: 0.023780447710305453 . Test loss: 1.6270051163144883\n",
      "Epoch: 87 completed in: 5.799116849899292 s. Training loss: 0.018737809018768967 . Val loss: 0.02375318673439324 . Test loss: 1.6250903517106927\n",
      "Epoch: 88 completed in: 5.850732803344727 s. Training loss: 0.018713563279568277 . Val loss: 0.023764185141772032 . Test loss: 1.6283663941195252\n",
      "Epoch: 89 completed in: 5.79066014289856 s. Training loss: 0.018680658918825813 . Val loss: 0.023754832288250326 . Test loss: 1.625958119618783\n",
      "Epoch: 90 completed in: 6.008426666259766 s. Training loss: 0.018653629504035422 . Val loss: 0.023734255321323873 . Test loss: 1.6270079443705638\n",
      "4.944182354829473e-05\n",
      "Epoch: 91 completed in: 5.804399013519287 s. Training loss: 0.018643877135902424 . Val loss: 0.02372469105757773 . Test loss: 1.6281797632317156\n",
      "Epoch: 92 completed in: 5.9816529750823975 s. Training loss: 0.018616396804632496 . Val loss: 0.023713668202981354 . Test loss: 1.6262542975305303\n",
      "Epoch: 93 completed in: 5.804969549179077 s. Training loss: 0.018610542558514288 . Val loss: 0.02370655843988061 . Test loss: 1.6265384781359584\n",
      "Epoch: 94 completed in: 6.020343065261841 s. Training loss: 0.01861535942684995 . Val loss: 0.023692084616050123 . Test loss: 1.6278206336532735\n",
      "Epoch: 95 completed in: 5.780343055725098 s. Training loss: 0.01856660113132892 . Val loss: 0.023685300489887595 . Test loss: 1.6279602411000604\n",
      "3.825714057690906e-05\n",
      "Epoch: 96 completed in: 5.907871246337891 s. Training loss: 0.018548954730296004 . Val loss: 0.023678818019106984 . Test loss: 1.6250830457063674\n",
      "Epoch: 97 completed in: 5.821540117263794 s. Training loss: 0.018547698461117534 . Val loss: 0.023669672338292 . Test loss: 1.6268597867357075\n",
      "Epoch: 98 completed in: 5.852774143218994 s. Training loss: 0.018562642313594366 . Val loss: 0.023658921103924514 . Test loss: 1.627688987905017\n",
      "Epoch: 99 completed in: 5.834981203079224 s. Training loss: 0.01852607355201549 . Val loss: 0.02366231186315417 . Test loss: 1.62650861359877\n",
      "Epoch: 100 completed in: 5.806021690368652 s. Training loss: 0.018513989376488828 . Val loss: 0.023657087283208967 . Test loss: 1.6268676978750747\n",
      "2.9602646101669974e-05\n",
      "1.6268676978750747\n"
     ]
    }
   ],
   "source": [
    "# train model \n",
    "\n",
    "trial_num = 3  \n",
    "preds_total = [] \n",
    "test_losses = [] \n",
    "num_epoch = 100 \n",
    "\n",
    "for i in range(trial_num): \n",
    "    # build model \n",
    "#     model = Seq2Seq_Attn(input_dim = 234, hidden_dim = 512, output_dim = 234, num_layers = 1, device = device).to(device) \n",
    "#     model = Seq2Seq(input_dim = 60, hidden_dim = 512, output_dim = 60, num_layers = 1).to(device) \n",
    "    model = FC(input_dim = 60, input_len = 12, hidden_dim = 512, output_dim = 60).to(device) \n",
    "#     model = Latent_ODE(latent_dim = 256, obs_dim = 240, nhidden = 512, rhidden = 512, aug = False).to(device)\n",
    "    name = \"FC\"\n",
    "    learning_rate = 0.005\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 1, gamma=0.95)\n",
    "    criterion = nn.MSELoss()\n",
    "    print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "    print(\"------Trial\", i + 1)\n",
    "    best_loss = 100   \n",
    "    train_losses = []\n",
    "    val_losses = [] \n",
    "    \n",
    "    for epoch in range(1, num_epoch + 1): \n",
    "        start = time.time()\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion)[-1]\n",
    "        train_losses.append(train_loss)\n",
    "        _, _, val_loss = eval_epoch(model, val_loader, criterion) \n",
    "        val_losses.append(val_loss)\n",
    "        if val_loss <= best_loss: \n",
    "            best_loss = val_loss \n",
    "            best_model = model \n",
    "#             torch.save({\"preds\": preds, \"trues\": trues, \"model\": best_model}, \"best_time_\" + name + str(i+1) + \".pt\") \n",
    "        end = time.time()\n",
    "        preds, trues, test_loss = eval_epoch_true(best_model, test_loader, criterion, std, mean) \n",
    "        print(\"Epoch:\", epoch, \"completed in:\", (end - start), \"s. Training loss:\", train_loss, \". Val loss:\", val_loss, \". Test loss:\", test_loss) \n",
    "        if (len(train_losses) > 50 and np.mean(val_losses[-5:]) >= np.mean(val_losses[-10:-5])):\n",
    "            break\n",
    "        scheduler.step() \n",
    "        if epoch % 5 == 0: print(optimizer.param_groups[0]['lr']) \n",
    "    \n",
    "    # save the best model, prediction and ground truth \n",
    "    preds, trues, test_loss = eval_epoch_true(best_model, test_loader, criterion, std, mean) \n",
    "    torch.save({\"preds\": preds, \"trues\": trues, \"model\": best_model}, \"result/FC/best_space_region\" + name + str(i) + \".pt\") \n",
    "    preds_total.append(preds)\n",
    "    test_losses.append(test_loss)\n",
    "    print(test_loss)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c1aa78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, trues, test_loss = eval_epoch_true(best_model, test_loader, criterion, std, mean) \n",
    "torch.save({\"preds\": preds, \"trues\": trues, \"model\": best_model}, \"result_bay/Seq2Seq/best_time_\" + name + str(2) + \".pt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce002ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((1, 512, 1024))\n",
    "b = torch.zeros((1, 512, 1024))\n",
    "torch.cat((a, b), dim = 2).shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9901be",
   "metadata": {},
   "source": [
    "### Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e7407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_losses = []\n",
    "for preds_i in preds_total: \n",
    "#     print(preds_i.shape)\n",
    "    rmse_losses.append(torch.sqrt(criterion(torch.tensor(preds_i), torch.tensor(trues))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e6e518",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43846394",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(rmse_losses).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06e8f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensor location \n",
    "\n",
    "xi = [0,  45,  56,  75,  81,  86,  89,  95, 100, 105, 109, 112, 117,\n",
    "       124, 128, 133, 137, 141, 146, 149, 152, 158, 163, 167, 171, 174,\n",
    "       180, 186, 192, 197, 200, 205, 207, 210, 211, 213, 214, 228, 231,\n",
    "       237, 240, 242, 251, 254, 258, 262, 266, 270, 277, 279, 282, 283,\n",
    "       286, 288, 291, 294, 296, 298, 300, 303, 308, 310, 315, 317, 320,\n",
    "       322, 327, 338, 342, 345, 352, 356, 359, 362, 366, 368, 374, 379] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9041331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the last time step from the first test sample \n",
    "\n",
    "preds_t = np.stack(preds_total)\n",
    "preds_mean = preds_t.mean(axis = 0)\n",
    "gt = trues[0]\n",
    "loss = np.array(test_losses).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6406920",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_1 = preds_t.std(axis = 0)[-1, 0, -1]\n",
    "std_2 = preds_t.std(axis = 0)[-1, 1, -1]\n",
    "std_3 = preds_t.std(axis = 0)[-1, 2, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa21350",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = preds_mean[-1, :, -1, :]\n",
    "std = [std_1, std_2, std_3]\n",
    "true = trues[-1][:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01062548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tp=24\n",
    "plt.style.use('default')\n",
    "label={0: \"Density $k$ (veh/m)\",\n",
    "       1: \"Flow $q$ (veh/s)\",\n",
    "       2: \"Speed $u$ (m/s)\"} \n",
    "\n",
    "from matplotlib import gridspec\n",
    "\n",
    "fwyp=[xi[n]* 300 / 1.e3 for n in range(len(xi))]\n",
    "\n",
    "#gs = gridspec.GridSpec(3, 1, height_ratios=[2, 1, 1]) \n",
    "gs_kw={\"height_ratios\": [3, 3, 3]}\n",
    "\n",
    "fig4,ax4=plt.subplots(figsize=(5, 15), nrows=3, gridspec_kw=gs_kw, sharex=True)\n",
    "fig4.subplots_adjust(left=0.08, bottom=0.08, right=0.98, top=0.95)\n",
    "ymin=[-0.01, -0.1, 20]\n",
    "ymax=[0.06, 1.5, 40]\n",
    "\n",
    "\n",
    "for n in range(3): \n",
    "    ax4[n].plot(fwyp, (true[n, :]), label='observed', marker='o', markersize=5)\n",
    "    ax4[n].plot(fwyp, (result[n, :]), label='predicted', marker='o', markersize=5)\n",
    "    ax4[n].fill_between(fwyp, (result[n, :] - std[n]), (result[n, :] + std[n]),  color='red', alpha=.3)    \n",
    "    ax4[n].set_ylim(ymin[n], ymax[n])\n",
    "    #ax4[n].set_xlabel(\"Distance (grid points)\")\n",
    "    ax4[n].set_ylabel(label[n])\n",
    "\n",
    "    #ax4[n].set_xlim(fwyp[0]-1, fwyp[-1]+1)\n",
    "\n",
    "#     lastpoint=-100\n",
    "#     if n==0:\n",
    "#         for p in range(len(xi)):\n",
    "#             if (fwyp[p] - lastpoint) > 1.:\n",
    "#                 ax4[n].text(fwyp[p], (y_pred[n, :, tp-68])[p] + 0.02, \"none\", rotation=90, fontsize=8)\n",
    "#                 lastpoint=fwyp[p]\n",
    "ax4[0].set_title(\"FC: last timestep from the last test sample\")    \n",
    "ax4[2].set_xlabel(\"Relative distance from North to South (km)\")\n",
    "#ax4[n].set_ylabel(\"Traffic density (Veh/m)\")\n",
    "fig4.savefig('FC.jpg', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
