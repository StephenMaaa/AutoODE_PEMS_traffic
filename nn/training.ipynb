{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9997bf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "import time\n",
    "from DNN import Dataset, train_epoch, eval_epoch, eval_epoch_true, FC, Seq2Seq, Seq2Seq_Attn \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a3ddf2",
   "metadata": {},
   "source": [
    "### Data for time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae599ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data = torch.load(\"data_time/pems_bay/training_time.pt\").float()\n",
    "# test_data = torch.load(\"data_time/pems_bay/test_time.pt\").float()\n",
    "# training_data = torch.load(\"data_time/pems_sd/training_time.pt\").float()\n",
    "# test_data = torch.load(\"data_time/pems_sd/test_time.pt\").float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d429d123",
   "metadata": {},
   "source": [
    "### Data for space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98169690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data = torch.load(\"data_space/training_space.pt\").float()\n",
    "# test_data = torch.load(\"data_space/test_space.pt\").float()\n",
    "training_data = torch.load(\"data_space/training_space_region.pt\").float()\n",
    "test_data = torch.load(\"data_space/test_space_region.pt\").float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30843848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([159768, 3, 24, 20]), torch.Size([53256, 3, 24, 20]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.shape, test_data.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5f06d8",
   "metadata": {},
   "source": [
    "### Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "923e1df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.tensor([training_data[:, 0, :, :].mean(), training_data[:, 1, :, :].mean(), training_data[:, 2, :, :].mean()])\n",
    "std = torch.tensor([training_data[:, 0, :, :].std(), training_data[:, 1, :, :].std(), training_data[:, 2, :, :].std()])\n",
    "training_norm = torch.zeros(training_data.shape, dtype = torch.float) \n",
    "training_norm[:, 0, :, :] = (training_data[:, 0, :, :] - mean[0]) / std[0]\n",
    "training_norm[:, 1, :, :] = (training_data[:, 1, :, :] - mean[1]) / std[1]\n",
    "training_norm[:, 2, :, :] = (training_data[:, 2, :, :] - mean[2]) / std[2] \n",
    "\n",
    "test_norm = torch.zeros(test_data.shape, dtype = torch.float) \n",
    "test_norm[:, 0, :, :] = (test_data[:, 0, :, :] - mean[0]) / std[0]\n",
    "test_norm[:, 1, :, :] = (test_data[:, 1, :, :] - mean[1]) / std[1]\n",
    "test_norm[:, 2, :, :] = (test_data[:, 2, :, :] - mean[2]) / std[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "511eeb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = Dataset(training_norm)\n",
    "test_set = Dataset(test_norm) \n",
    "training_set, val_set = data.random_split(training_set, [int(len(training_set) * 0.875), int(len(training_set) - int(len(training_set) * 0.875))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a61796bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader \n",
    "train_loader = data.DataLoader(training_set, batch_size = 512, shuffle = True)\n",
    "val_loader = data.DataLoader(val_set, batch_size = 512, shuffle = False) \n",
    "test_loader = data.DataLoader(test_set, batch_size = 512, shuffle = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2b0b282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139797\n",
      "19971\n",
      "53256\n"
     ]
    }
   ],
   "source": [
    "print(len(training_set))\n",
    "print(len(val_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af75743",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19ba19ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6938980\n",
      "------Trial 1\n",
      "Epoch: 1 completed in: 12.939138174057007 s. Training loss: 0.2728724391845456 . Val loss: 0.18476229533553123 . Test loss: 1.629836395411612\n",
      "Epoch: 2 completed in: 12.683636665344238 s. Training loss: 0.13175742765956552 . Val loss: 0.12771276161074638 . Test loss: 1.376729307320319\n",
      "Epoch: 3 completed in: 12.655012607574463 s. Training loss: 0.11363615745501797 . Val loss: 0.11912829447537661 . Test loss: 1.3345872110661796\n",
      "Epoch: 4 completed in: 12.824986219406128 s. Training loss: 0.10534510984472985 . Val loss: 0.10404864363372326 . Test loss: 1.3467061493493138\n",
      "Epoch: 5 completed in: 12.911186218261719 s. Training loss: 0.09797057887389712 . Val loss: 0.1052913449704647 . Test loss: 1.31489837302477\n",
      "0.007737809374999999\n",
      "Epoch: 6 completed in: 12.877658605575562 s. Training loss: 0.09283876254556388 . Val loss: 0.09621848333626985 . Test loss: 1.308874728654446\n",
      "Epoch: 7 completed in: 12.92129921913147 s. Training loss: 0.09152788490763981 . Val loss: 0.09920365158468485 . Test loss: 1.340066135756335\n",
      "Epoch: 8 completed in: 12.932788133621216 s. Training loss: 0.08888840582901544 . Val loss: 0.10481185987591743 . Test loss: 1.3823606854171115\n",
      "Epoch: 9 completed in: 12.983301401138306 s. Training loss: 0.0879208792517655 . Val loss: 0.09291218910366297 . Test loss: 1.3500651421422092\n",
      "Epoch: 10 completed in: 13.354596614837646 s. Training loss: 0.08285769585003383 . Val loss: 0.08773654885590076 . Test loss: 1.3584492645987822\n",
      "0.005987369392383786\n",
      "Epoch: 11 completed in: 13.186060905456543 s. Training loss: 0.08009742491327933 . Val loss: 0.08755493611097336 . Test loss: 1.3110325976095885\n",
      "Epoch: 12 completed in: 13.152512550354004 s. Training loss: 0.08397037960099478 . Val loss: 0.0878550024703145 . Test loss: 1.4122932011928495\n",
      "Epoch: 13 completed in: 13.166399002075195 s. Training loss: 0.07763403091226181 . Val loss: 0.08074540290981531 . Test loss: 1.3691896596051807\n",
      "Epoch: 14 completed in: 13.113891124725342 s. Training loss: 0.07488163190819051 . Val loss: 0.08238872233778238 . Test loss: 1.40790406446427\n",
      "Epoch: 15 completed in: 13.210805892944336 s. Training loss: 0.07339201360451479 . Val loss: 0.07821944281458855 . Test loss: 1.4883723226935215\n",
      "0.00463291230159753\n",
      "Epoch: 16 completed in: 13.209869861602783 s. Training loss: 0.06988106461337013 . Val loss: 0.07525759935379028 . Test loss: 1.4432065255878812\n",
      "Epoch: 17 completed in: 13.19167947769165 s. Training loss: 0.06770661149690621 . Val loss: 0.0717282710596919 . Test loss: 1.481273052372675\n",
      "Epoch: 18 completed in: 13.306868314743042 s. Training loss: 0.06557311683240598 . Val loss: 0.07963630501180888 . Test loss: 1.4856421968716658\n",
      "Epoch: 19 completed in: 13.238056182861328 s. Training loss: 0.06465439374701384 . Val loss: 0.06719578579068183 . Test loss: 1.552600282383991\n",
      "Epoch: 20 completed in: 13.33646011352539 s. Training loss: 0.061249913335064035 . Val loss: 0.06768876053392887 . Test loss: 1.4652171118229094\n",
      "0.0035848592240854188\n",
      "Epoch: 21 completed in: 13.197303295135498 s. Training loss: 0.060235337116313674 . Val loss: 0.06422217534855008 . Test loss: 1.4529042395134597\n",
      "Epoch: 22 completed in: 13.286885261535645 s. Training loss: 0.05778469951537839 . Val loss: 0.06352363657206297 . Test loss: 1.5494814856314585\n",
      "Epoch: 23 completed in: 13.283195495605469 s. Training loss: 0.05624387671586371 . Val loss: 0.060697140730917455 . Test loss: 1.5325776460058027\n",
      "Epoch: 24 completed in: 13.206284761428833 s. Training loss: 0.05406125785823721 . Val loss: 0.05948749417439103 . Test loss: 1.509743000923894\n",
      "Epoch: 25 completed in: 13.32621145248413 s. Training loss: 0.051676131281865774 . Val loss: 0.0568856087513268 . Test loss: 1.5125714387465152\n",
      "0.0027738957312183374\n",
      "Epoch: 26 completed in: 13.243345260620117 s. Training loss: 0.049672622437568474 . Val loss: 0.05526761412620544 . Test loss: 1.4933443256885361\n",
      "Epoch: 27 completed in: 13.321025133132935 s. Training loss: 0.04814892727201873 . Val loss: 0.054322910588234664 . Test loss: 1.5698003787422958\n",
      "Epoch: 28 completed in: 13.174070835113525 s. Training loss: 0.046525924500540224 . Val loss: 0.052507102210074666 . Test loss: 1.5269681696636301\n",
      "Epoch: 29 completed in: 13.242219686508179 s. Training loss: 0.044794491349454346 . Val loss: 0.05037740739062428 . Test loss: 1.510902076372065\n",
      "Epoch: 30 completed in: 13.202778816223145 s. Training loss: 0.042817796138625075 . Val loss: 0.049614892713725565 . Test loss: 1.53402082321368\n",
      "0.0021463876394293723\n",
      "Epoch: 31 completed in: 13.204873085021973 s. Training loss: 0.0413036062008273 . Val loss: 0.0478861746378243 . Test loss: 1.5423535423929116\n",
      "Epoch: 32 completed in: 13.116696119308472 s. Training loss: 0.039781254559864095 . Val loss: 0.04632695475593209 . Test loss: 1.5500046283807372\n",
      "Epoch: 33 completed in: 13.126451015472412 s. Training loss: 0.038773206776402294 . Val loss: 0.04591359039768576 . Test loss: 1.5769926998648238\n",
      "Epoch: 34 completed in: 13.134467124938965 s. Training loss: 0.0373889968533368 . Val loss: 0.04427876779809594 . Test loss: 1.5712155252837237\n",
      "Epoch: 35 completed in: 13.196369409561157 s. Training loss: 0.03623907011083878 . Val loss: 0.04276951923966408 . Test loss: 1.5766406125294439\n",
      "0.0016608338398760713\n",
      "Epoch: 36 completed in: 13.116865634918213 s. Training loss: 0.03501062885089947 . Val loss: 0.04233210254460573 . Test loss: 1.579211453861813\n",
      "Epoch: 37 completed in: 13.149258613586426 s. Training loss: 0.03376740660436832 . Val loss: 0.04034775644540787 . Test loss: 1.5772152469504304\n",
      "Epoch: 38 completed in: 13.13487720489502 s. Training loss: 0.033194917498877013 . Val loss: 0.04023673618212342 . Test loss: 1.5638600381302115\n",
      "Epoch: 39 completed in: 13.146198034286499 s. Training loss: 0.03220799784210041 . Val loss: 0.03896225430071354 . Test loss: 1.5809305856657443\n",
      "Epoch: 40 completed in: 13.398686170578003 s. Training loss: 0.031209158507196137 . Val loss: 0.037609112542122604 . Test loss: 1.5800472910146728\n",
      "0.0012851215656510308\n",
      "Epoch: 41 completed in: 13.362269639968872 s. Training loss: 0.030285199471905718 . Val loss: 0.037562439776957036 . Test loss: 1.5751489229449138\n",
      "Epoch: 42 completed in: 13.219228267669678 s. Training loss: 0.029835601595577098 . Val loss: 0.03676136871799827 . Test loss: 1.5949415466295613\n",
      "Epoch: 43 completed in: 13.17555856704712 s. Training loss: 0.02884537679734674 . Val loss: 0.036291486117988825 . Test loss: 1.5856675556963657\n",
      "Epoch: 44 completed in: 13.23548698425293 s. Training loss: 0.02838748581299599 . Val loss: 0.03524375087581575 . Test loss: 1.5903631318190492\n",
      "Epoch: 45 completed in: 13.281522512435913 s. Training loss: 0.02762316395086746 . Val loss: 0.03531747837550938 . Test loss: 1.6010867591878346\n",
      "0.000994402569870922\n",
      "Epoch: 46 completed in: 13.241722345352173 s. Training loss: 0.02733392477117098 . Val loss: 0.03448493550531566 . Test loss: 1.6065594670667094\n",
      "Epoch: 47 completed in: 13.226881504058838 s. Training loss: 0.026503195972555744 . Val loss: 0.03384477030485868 . Test loss: 1.5931750719111633\n",
      "Epoch: 48 completed in: 13.594594955444336 s. Training loss: 0.025982718710807987 . Val loss: 0.03348435116931796 . Test loss: 1.6068820897039022\n",
      "Epoch: 49 completed in: 13.20480751991272 s. Training loss: 0.025678327674195714 . Val loss: 0.032882234267890456 . Test loss: 1.588954402079505\n",
      "Epoch: 50 completed in: 13.287379741668701 s. Training loss: 0.025232085903739408 . Val loss: 0.03254897906444967 . Test loss: 1.575514785938149\n",
      "0.0007694497527671312\n",
      "Epoch: 51 completed in: 13.296948194503784 s. Training loss: 0.02473890705265268 . Val loss: 0.03195286155678332 . Test loss: 1.5980665509036274\n",
      "Epoch: 52 completed in: 13.279996156692505 s. Training loss: 0.02426917530106802 . Val loss: 0.03201110502704978 . Test loss: 1.5954947290458419\n",
      "Epoch: 53 completed in: 13.186059474945068 s. Training loss: 0.023991679024957392 . Val loss: 0.03140090904198587 . Test loss: 1.5979825959317608\n",
      "Epoch: 54 completed in: 13.236783027648926 s. Training loss: 0.023617698049621424 . Val loss: 0.031247045332565903 . Test loss: 1.5996359531428723\n",
      "Epoch: 55 completed in: 13.327322244644165 s. Training loss: 0.02334805708514513 . Val loss: 0.03089625868014991 . Test loss: 1.6058891417793004\n",
      "0.000595385551055294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56 completed in: 13.227051734924316 s. Training loss: 0.023009125483188318 . Val loss: 0.030743526248261332 . Test loss: 1.6136313103596776\n",
      "Epoch: 57 completed in: 13.31382131576538 s. Training loss: 0.022829327453607624 . Val loss: 0.030301398038864134 . Test loss: 1.602203489688986\n",
      "Epoch: 58 completed in: 13.284939527511597 s. Training loss: 0.022451513283716067 . Val loss: 0.030198209453374147 . Test loss: 1.600114384488949\n",
      "Epoch: 59 completed in: 13.284379482269287 s. Training loss: 0.02220173318781992 . Val loss: 0.029903429187834264 . Test loss: 1.6072787374686275\n",
      "Epoch: 60 completed in: 13.277870655059814 s. Training loss: 0.021993643757173397 . Val loss: 0.029667142778635025 . Test loss: 1.6166917211635223\n",
      "0.00046069798986951934\n",
      "Epoch: 61 completed in: 13.245464563369751 s. Training loss: 0.021702700444109685 . Val loss: 0.029588049137964846 . Test loss: 1.6046083308389265\n",
      "Epoch: 62 completed in: 13.265088319778442 s. Training loss: 0.02158395049372946 . Val loss: 0.029608676955103875 . Test loss: 1.6103382582297912\n",
      "Epoch: 63 completed in: 13.261342525482178 s. Training loss: 0.02136003427261854 . Val loss: 0.029245705204084517 . Test loss: 1.6056483949091997\n",
      "Epoch: 64 completed in: 13.267929553985596 s. Training loss: 0.021185073308157224 . Val loss: 0.02907681968063116 . Test loss: 1.609205249188279\n",
      "Epoch: 65 completed in: 13.228261470794678 s. Training loss: 0.021005652321461777 . Val loss: 0.02880024928599596 . Test loss: 1.6143362622996595\n",
      "0.00035647932250560207\n",
      "Epoch: 66 completed in: 13.34828805923462 s. Training loss: 0.020814458234575544 . Val loss: 0.02878027278929949 . Test loss: 1.6134362606252355\n",
      "Epoch: 67 completed in: 13.252036094665527 s. Training loss: 0.020666068630551336 . Val loss: 0.02860218328423798 . Test loss: 1.6156138010647922\n",
      "Epoch: 68 completed in: 13.546064615249634 s. Training loss: 0.02050985525749678 . Val loss: 0.028606823831796645 . Test loss: 1.6184972622219433\n",
      "Epoch: 69 completed in: 13.270944356918335 s. Training loss: 0.020354018679201384 . Val loss: 0.02839060132391751 . Test loss: 1.6176132792108928\n",
      "Epoch: 70 completed in: 13.486354112625122 s. Training loss: 0.020187449288031047 . Val loss: 0.02825601599179208 . Test loss: 1.6181332980559575\n",
      "0.00027583690436774953\n",
      "Epoch: 71 completed in: 13.289965152740479 s. Training loss: 0.020077398543103333 . Val loss: 0.02820394402369857 . Test loss: 1.6138489035818706\n",
      "Epoch: 72 completed in: 13.346407651901245 s. Training loss: 0.019980827557181356 . Val loss: 0.02807143535465002 . Test loss: 1.6122988333357464\n",
      "Epoch: 73 completed in: 13.649787902832031 s. Training loss: 0.019846700376620258 . Val loss: 0.02797780972905457 . Test loss: 1.6170668944673985\n",
      "Epoch: 74 completed in: 13.31795620918274 s. Training loss: 0.01974004260298327 . Val loss: 0.027979922387748955 . Test loss: 1.6161028177127945\n",
      "Epoch: 75 completed in: 13.3142991065979 s. Training loss: 0.019617168742646702 . Val loss: 0.027820392185822128 . Test loss: 1.6212079360850464\n",
      "0.00021343733845877503\n",
      "Epoch: 76 completed in: 13.297928094863892 s. Training loss: 0.0195088436160862 . Val loss: 0.027775785606354474 . Test loss: 1.6188870638306372\n",
      "Epoch: 77 completed in: 13.284617185592651 s. Training loss: 0.019450817556276808 . Val loss: 0.027740913070738317 . Test loss: 1.624442313200275\n",
      "Epoch: 78 completed in: 13.309805631637573 s. Training loss: 0.01936193399919863 . Val loss: 0.027692682389169932 . Test loss: 1.619358745517982\n",
      "Epoch: 79 completed in: 13.311699628829956 s. Training loss: 0.01926084066040977 . Val loss: 0.027647095406427978 . Test loss: 1.6217726757693798\n",
      "Epoch: 80 completed in: 13.249936580657959 s. Training loss: 0.01918614524532191 . Val loss: 0.02758610239252448 . Test loss: 1.6246208652525946\n",
      "0.00016515374385013573\n",
      "Epoch: 81 completed in: 13.63016152381897 s. Training loss: 0.019097960411305844 . Val loss: 0.027503914153203367 . Test loss: 1.6225577822567692\n",
      "Epoch: 82 completed in: 13.85967493057251 s. Training loss: 0.019033179013398443 . Val loss: 0.027457470539957285 . Test loss: 1.6232757956030337\n",
      "Epoch: 83 completed in: 13.832849502563477 s. Training loss: 0.018940809838827292 . Val loss: 0.02741242842748761 . Test loss: 1.6234701450114253\n",
      "Epoch: 84 completed in: 13.193778038024902 s. Training loss: 0.018876639133604774 . Val loss: 0.027391632553189993 . Test loss: 1.628819686089808\n",
      "Epoch: 85 completed in: 13.384274005889893 s. Training loss: 0.0188243161193537 . Val loss: 0.027360396832227706 . Test loss: 1.6247520552803532\n",
      "0.00012779281874799285\n",
      "Epoch: 86 completed in: 13.258796691894531 s. Training loss: 0.01878115482438002 . Val loss: 0.027362694405019282 . Test loss: 1.6243878872765438\n",
      "Epoch: 87 completed in: 13.262514591217041 s. Training loss: 0.018728357868908096 . Val loss: 0.027263647178187966 . Test loss: 1.6251643027918126\n",
      "Epoch: 88 completed in: 13.244126081466675 s. Training loss: 0.018655163161184665 . Val loss: 0.027229962544515728 . Test loss: 1.6236489230189954\n",
      "Epoch: 89 completed in: 13.25084137916565 s. Training loss: 0.018608734215589336 . Val loss: 0.027215085970237852 . Test loss: 1.6245551334400001\n",
      "Epoch: 90 completed in: 13.576097011566162 s. Training loss: 0.01856474339771662 . Val loss: 0.02716516931541264 . Test loss: 1.6256284625934097\n",
      "9.888364709658946e-05\n",
      "Epoch: 91 completed in: 13.287123680114746 s. Training loss: 0.0185043712479681 . Val loss: 0.02713564862497151 . Test loss: 1.6241373425526577\n",
      "Epoch: 92 completed in: 13.31100058555603 s. Training loss: 0.01846146760304479 . Val loss: 0.02713042716495693 . Test loss: 1.6258579467816605\n",
      "Epoch: 93 completed in: 13.312946319580078 s. Training loss: 0.01841808886804285 . Val loss: 0.027119546523317696 . Test loss: 1.627530336838103\n",
      "Epoch: 94 completed in: 13.274659633636475 s. Training loss: 0.0183873245098295 . Val loss: 0.02707279296591878 . Test loss: 1.6287208011590293\n",
      "Epoch: 95 completed in: 13.260144233703613 s. Training loss: 0.018342771574202246 . Val loss: 0.02705947975628078 . Test loss: 1.62585114128307\n",
      "7.651428115381812e-05\n",
      "Epoch: 96 completed in: 13.253599882125854 s. Training loss: 0.0182968013083739 . Val loss: 0.027021364448592065 . Test loss: 1.6280458023645061\n",
      "Epoch: 97 completed in: 13.30293345451355 s. Training loss: 0.018274730758020913 . Val loss: 0.02700513103045523 . Test loss: 1.6265564343041967\n",
      "Epoch: 98 completed in: 13.274290800094604 s. Training loss: 0.01825862737494881 . Val loss: 0.027002617390826343 . Test loss: 1.6264031297198747\n",
      "Epoch: 99 completed in: 13.246487140655518 s. Training loss: 0.01820997474035316 . Val loss: 0.02698270226828754 . Test loss: 1.6257540466800868\n",
      "Epoch: 100 completed in: 13.257763624191284 s. Training loss: 0.01819451010520876 . Val loss: 0.026953348098322748 . Test loss: 1.6274554329479765\n",
      "5.920529220333995e-05\n",
      "1.6274554329479765\n",
      "6938980\n",
      "------Trial 2\n",
      "Epoch: 1 completed in: 13.239289045333862 s. Training loss: 0.2222571518734424 . Val loss: 0.12880139984190464 . Test loss: 1.3918434837093498\n",
      "Epoch: 2 completed in: 13.103224754333496 s. Training loss: 0.11119905791252199 . Val loss: 0.1081648476421833 . Test loss: 1.2564028902538835\n",
      "Epoch: 3 completed in: 13.27063536643982 s. Training loss: 0.09851082884808526 . Val loss: 0.10566732361912727 . Test loss: 1.2568864498121393\n",
      "Epoch: 4 completed in: 13.188315629959106 s. Training loss: 0.09183625466305845 . Val loss: 0.09218518957495689 . Test loss: 1.2250336410483427\n",
      "Epoch: 5 completed in: 13.221293210983276 s. Training loss: 0.08573980290904967 . Val loss: 0.08907361999154091 . Test loss: 1.283763076143258\n",
      "0.007737809374999999\n",
      "Epoch: 6 completed in: 13.072400331497192 s. Training loss: 0.08183137767941412 . Val loss: 0.08601498696953058 . Test loss: 1.2837157973068767\n",
      "Epoch: 7 completed in: 13.135294437408447 s. Training loss: 0.07884476290349543 . Val loss: 0.08288703169673681 . Test loss: 1.2929775773911756\n",
      "Epoch: 8 completed in: 13.279454231262207 s. Training loss: 0.07541770390132918 . Val loss: 0.08602597042918206 . Test loss: 1.2438966048537548\n",
      "Epoch: 9 completed in: 13.161508798599243 s. Training loss: 0.07351080465545184 . Val loss: 0.07600086089223623 . Test loss: 1.2782711227329966\n",
      "Epoch: 10 completed in: 13.121633052825928 s. Training loss: 0.07054642949552431 . Val loss: 0.07276379633694888 . Test loss: 1.322573692500176\n",
      "0.005987369392383786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 completed in: 13.153262853622437 s. Training loss: 0.06734979434115608 . Val loss: 0.07347200382500887 . Test loss: 1.448272498880203\n",
      "Epoch: 12 completed in: 13.15029764175415 s. Training loss: 0.06568099683436164 . Val loss: 0.06833633864298463 . Test loss: 1.3664702198093812\n",
      "Epoch: 13 completed in: 13.16793155670166 s. Training loss: 0.06290486159931569 . Val loss: 0.06737783569842577 . Test loss: 1.463687484900175\n",
      "Epoch: 14 completed in: 13.311776638031006 s. Training loss: 0.0597538969812602 . Val loss: 0.06551543166860938 . Test loss: 1.4008836554637503\n",
      "Epoch: 15 completed in: 13.167620420455933 s. Training loss: 0.05833538865031552 . Val loss: 0.06416697949171066 . Test loss: 1.3310890019808237\n",
      "0.00463291230159753\n",
      "Epoch: 16 completed in: 13.10506010055542 s. Training loss: 0.05499936614430299 . Val loss: 0.05863631283864379 . Test loss: 1.4130540654183483\n",
      "Epoch: 17 completed in: 13.14641809463501 s. Training loss: 0.051956002824824224 . Val loss: 0.05702039776369929 . Test loss: 1.4555588658766183\n",
      "Epoch: 18 completed in: 13.202199459075928 s. Training loss: 0.05037320666287067 . Val loss: 0.05653325468301773 . Test loss: 1.4640634275602997\n",
      "Epoch: 19 completed in: 13.231041669845581 s. Training loss: 0.04914436754464668 . Val loss: 0.05402054823935032 . Test loss: 1.6059460397323844\n",
      "Epoch: 20 completed in: 13.262166023254395 s. Training loss: 0.04757949099434118 . Val loss: 0.0522955839522183 . Test loss: 1.538803187224658\n",
      "0.0035848592240854188\n",
      "Epoch: 21 completed in: 13.461394548416138 s. Training loss: 0.044283925358505144 . Val loss: 0.047909802105277774 . Test loss: 1.5188143519459818\n",
      "Epoch: 22 completed in: 13.638752937316895 s. Training loss: 0.041930369924019724 . Val loss: 0.04768616613000631 . Test loss: 1.460369950917425\n",
      "Epoch: 23 completed in: 13.31826639175415 s. Training loss: 0.04076379333643147 . Val loss: 0.044595465436577796 . Test loss: 1.4807542037067547\n",
      "Epoch: 24 completed in: 13.33977723121643 s. Training loss: 0.038627085989735424 . Val loss: 0.04385584956035018 . Test loss: 1.490265314699611\n",
      "Epoch: 25 completed in: 13.287862062454224 s. Training loss: 0.03777317375107838 . Val loss: 0.042129564099013805 . Test loss: 1.5723279364896736\n",
      "0.0027738957312183374\n",
      "Epoch: 26 completed in: 13.328893899917603 s. Training loss: 0.03528167862091621 . Val loss: 0.04170047105289996 . Test loss: 1.5361792363397269\n",
      "Epoch: 27 completed in: 13.275346755981445 s. Training loss: 0.03449473246578535 . Val loss: 0.03949842136353254 . Test loss: 1.6120188415977605\n",
      "Epoch: 28 completed in: 13.264944076538086 s. Training loss: 0.03327835651019411 . Val loss: 0.03852266222238541 . Test loss: 1.5480933344305492\n",
      "Epoch: 29 completed in: 13.304636240005493 s. Training loss: 0.03142195012094113 . Val loss: 0.036766218952834606 . Test loss: 1.5440622807669306\n",
      "Epoch: 30 completed in: 13.197789192199707 s. Training loss: 0.030426908325213584 . Val loss: 0.036882282607257365 . Test loss: 1.5605967997879147\n",
      "0.0021463876394293723\n",
      "Epoch: 31 completed in: 13.259873151779175 s. Training loss: 0.03119894227244123 . Val loss: 0.03570501683279872 . Test loss: 1.5821707273488534\n",
      "Epoch: 32 completed in: 13.20137643814087 s. Training loss: 0.0287473170209105 . Val loss: 0.0342473731841892 . Test loss: 1.5692660290580316\n",
      "Epoch: 33 completed in: 13.2256600856781 s. Training loss: 0.027537552327135183 . Val loss: 0.03297182354144752 . Test loss: 1.575389595000323\n",
      "Epoch: 34 completed in: 13.267318964004517 s. Training loss: 0.02668480375904019 . Val loss: 0.03235694924369455 . Test loss: 1.5490032701530645\n",
      "Epoch: 35 completed in: 13.279390573501587 s. Training loss: 0.025882136076688766 . Val loss: 0.03180499291047454 . Test loss: 1.5672272280571162\n",
      "0.0016608338398760713\n",
      "Epoch: 36 completed in: 13.223946332931519 s. Training loss: 0.02537390599231215 . Val loss: 0.031061779148876666 . Test loss: 1.5492094317428207\n",
      "Epoch: 37 completed in: 13.317054986953735 s. Training loss: 0.024735624092991335 . Val loss: 0.030763398483395578 . Test loss: 1.5934652684552506\n",
      "Epoch: 38 completed in: 13.27403974533081 s. Training loss: 0.02440814953977174 . Val loss: 0.029802962485700846 . Test loss: 1.582279648294652\n",
      "Epoch: 39 completed in: 13.29053807258606 s. Training loss: 0.02327233970328404 . Val loss: 0.029171980964019895 . Test loss: 1.579265600993154\n",
      "Epoch: 40 completed in: 13.238028764724731 s. Training loss: 0.022717327946783418 . Val loss: 0.02853728267364204 . Test loss: 1.5883079846795465\n",
      "0.0012851215656510308\n",
      "Epoch: 41 completed in: 13.286889791488647 s. Training loss: 0.022184244951872278 . Val loss: 0.02800217978656292 . Test loss: 1.5860469432260407\n",
      "Epoch: 42 completed in: 13.24366569519043 s. Training loss: 0.0215718696628065 . Val loss: 0.027744668908417223 . Test loss: 1.5994132806209624\n",
      "Epoch: 43 completed in: 13.234143018722534 s. Training loss: 0.021311787405751483 . Val loss: 0.027606562618166207 . Test loss: 1.5968916202146293\n",
      "Epoch: 44 completed in: 13.23614501953125 s. Training loss: 0.020800260419066807 . Val loss: 0.027018908224999905 . Test loss: 1.5935363446893862\n",
      "Epoch: 45 completed in: 13.297940492630005 s. Training loss: 0.020373703184517197 . Val loss: 0.026442789332941176 . Test loss: 1.5832164442804042\n",
      "0.000994402569870922\n",
      "Epoch: 46 completed in: 13.256906509399414 s. Training loss: 0.019990328828511882 . Val loss: 0.026254908088594674 . Test loss: 1.5870496574628499\n",
      "Epoch: 47 completed in: 13.254440784454346 s. Training loss: 0.019661571694551593 . Val loss: 0.026116988714784385 . Test loss: 1.6064693981490223\n",
      "Epoch: 48 completed in: 13.280654191970825 s. Training loss: 0.01936674629249712 . Val loss: 0.025631550559774043 . Test loss: 1.6179431170153804\n",
      "Epoch: 49 completed in: 14.278982162475586 s. Training loss: 0.01900686507867853 . Val loss: 0.025337870186194777 . Test loss: 1.603209290217912\n",
      "Epoch: 50 completed in: 13.4613778591156 s. Training loss: 0.018725089392332483 . Val loss: 0.02505284729413688 . Test loss: 1.599108194934957\n",
      "0.0007694497527671312\n",
      "Epoch: 51 completed in: 13.29013967514038 s. Training loss: 0.018297676622432514 . Val loss: 0.024917771480977534 . Test loss: 1.6001308798301235\n",
      "Epoch: 52 completed in: 13.22329044342041 s. Training loss: 0.018060937564629707 . Val loss: 0.02470090533606708 . Test loss: 1.6127378524364853\n",
      "Epoch: 53 completed in: 13.160550594329834 s. Training loss: 0.017888828520629094 . Val loss: 0.02449245685711503 . Test loss: 1.585937522191999\n",
      "Epoch: 54 completed in: 13.306406021118164 s. Training loss: 0.01759353483326896 . Val loss: 0.024348529148846866 . Test loss: 1.6043139803497959\n",
      "Epoch: 55 completed in: 13.10700798034668 s. Training loss: 0.017502865822047648 . Val loss: 0.024177363701164724 . Test loss: 1.6069321304192974\n",
      "0.000595385551055294\n",
      "Epoch: 56 completed in: 13.225974798202515 s. Training loss: 0.017153875711951812 . Val loss: 0.023838657932356 . Test loss: 1.6085878125542148\n",
      "Epoch: 57 completed in: 13.16086459159851 s. Training loss: 0.016890910284145036 . Val loss: 0.023887748038396238 . Test loss: 1.5984173070051788\n",
      "Epoch: 58 completed in: 13.121829271316528 s. Training loss: 0.01672159730844254 . Val loss: 0.02361987014301121 . Test loss: 1.607750952064826\n",
      "Epoch: 59 completed in: 13.392986536026001 s. Training loss: 0.016525212967645947 . Val loss: 0.023534791264683008 . Test loss: 1.621082734307458\n",
      "Epoch: 60 completed in: 13.14301609992981 s. Training loss: 0.016403934677015907 . Val loss: 0.02340945219621062 . Test loss: 1.6114320261650843\n",
      "0.00046069798986951934\n",
      "Epoch: 61 completed in: 13.26879096031189 s. Training loss: 0.01622649366172017 . Val loss: 0.02329367185011506 . Test loss: 1.608472771765583\n",
      "Epoch: 62 completed in: 13.226409673690796 s. Training loss: 0.01603578899164487 . Val loss: 0.0232576762791723 . Test loss: 1.6179841214351696\n",
      "Epoch: 63 completed in: 13.238792896270752 s. Training loss: 0.015889528822697646 . Val loss: 0.0231226391158998 . Test loss: 1.6133821400680441\n",
      "Epoch: 64 completed in: 13.301030158996582 s. Training loss: 0.0157766213545399 . Val loss: 0.023051098641008138 . Test loss: 1.6133492684933173\n",
      "Epoch: 65 completed in: 13.26883053779602 s. Training loss: 0.01567239649117972 . Val loss: 0.02302592359483242 . Test loss: 1.6209129433139895\n",
      "0.00035647932250560207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66 completed in: 13.174572467803955 s. Training loss: 0.015528265233865402 . Val loss: 0.02283902750350535 . Test loss: 1.6162479391240474\n",
      "Epoch: 67 completed in: 13.601306676864624 s. Training loss: 0.01540345648290032 . Val loss: 0.02285602637566626 . Test loss: 1.6188216496980585\n",
      "Epoch: 68 completed in: 13.232865571975708 s. Training loss: 0.015306687465168698 . Val loss: 0.022775819431990384 . Test loss: 1.6214233196056758\n",
      "Epoch: 69 completed in: 13.263936519622803 s. Training loss: 0.015168141872778425 . Val loss: 0.02266347426921129 . Test loss: 1.6210417319710113\n",
      "Epoch: 70 completed in: 13.181136131286621 s. Training loss: 0.015048033676796803 . Val loss: 0.02262782109901309 . Test loss: 1.620790300884894\n",
      "0.00027583690436774953\n",
      "Epoch: 71 completed in: 13.221373081207275 s. Training loss: 0.01495220225617072 . Val loss: 0.02260178015567362 . Test loss: 1.6245585441051507\n",
      "Epoch: 72 completed in: 13.153115034103394 s. Training loss: 0.014883165862955098 . Val loss: 0.02256251536309719 . Test loss: 1.6294692366139745\n",
      "Epoch: 73 completed in: 13.333724737167358 s. Training loss: 0.014803277590332458 . Val loss: 0.022512246668338776 . Test loss: 1.636265154801027\n",
      "Epoch: 74 completed in: 13.185380935668945 s. Training loss: 0.014718763574441202 . Val loss: 0.0224870634265244 . Test loss: 1.6269196312948622\n",
      "Epoch: 75 completed in: 13.361391544342041 s. Training loss: 0.014638539940717012 . Val loss: 0.022429639380425213 . Test loss: 1.6352800104192897\n",
      "0.00021343733845877503\n",
      "Epoch: 76 completed in: 13.36112928390503 s. Training loss: 0.014576467076982678 . Val loss: 0.022417211811989544 . Test loss: 1.6325124049808994\n",
      "Epoch: 77 completed in: 13.319236993789673 s. Training loss: 0.014479012435886764 . Val loss: 0.022403816878795623 . Test loss: 1.6292705241398269\n",
      "Epoch: 78 completed in: 13.281649827957153 s. Training loss: 0.01440651829347667 . Val loss: 0.022327924985438584 . Test loss: 1.6272427419698157\n",
      "Epoch: 79 completed in: 13.222452163696289 s. Training loss: 0.014343851470272907 . Val loss: 0.022320935828611256 . Test loss: 1.6292995444627805\n",
      "Epoch: 80 completed in: 13.198060750961304 s. Training loss: 0.014288706320888587 . Val loss: 0.02229350693523884 . Test loss: 1.6340857482809346\n",
      "0.00016515374385013573\n",
      "Epoch: 81 completed in: 13.2780282497406 s. Training loss: 0.01423345672741641 . Val loss: 0.022278463654220104 . Test loss: 1.6271748713923302\n",
      "Epoch: 82 completed in: 13.294907093048096 s. Training loss: 0.0141803200159521 . Val loss: 0.022248522844165564 . Test loss: 1.6330949446473755\n",
      "Epoch: 83 completed in: 13.172160387039185 s. Training loss: 0.01413572914762436 . Val loss: 0.022208593180403115 . Test loss: 1.6332480063836272\n",
      "Epoch: 84 completed in: 13.169583082199097 s. Training loss: 0.014070610872666985 . Val loss: 0.022230785712599753 . Test loss: 1.6389021850356522\n",
      "Epoch: 85 completed in: 13.203458309173584 s. Training loss: 0.014033178639101938 . Val loss: 0.02221499220468104 . Test loss: 1.635171021689483\n",
      "0.00012779281874799285\n",
      "Epoch: 86 completed in: 13.322384357452393 s. Training loss: 0.013977921255150415 . Val loss: 0.02218121481128037 . Test loss: 1.6383055016000603\n",
      "Epoch: 87 completed in: 13.238329887390137 s. Training loss: 0.013932651058383231 . Val loss: 0.022164166159927844 . Test loss: 1.6354132234272738\n",
      "Epoch: 88 completed in: 13.282804489135742 s. Training loss: 0.013884013369135613 . Val loss: 0.02215579627081752 . Test loss: 1.6353254674914737\n",
      "Epoch: 89 completed in: 13.213139057159424 s. Training loss: 0.013846212838280157 . Val loss: 0.022161872778087854 . Test loss: 1.635381978818048\n",
      "Epoch: 90 completed in: 13.370860576629639 s. Training loss: 0.013813441010858239 . Val loss: 0.022137591103091835 . Test loss: 1.6313563116822147\n",
      "9.888364709658946e-05\n",
      "Epoch: 91 completed in: 13.388715505599976 s. Training loss: 0.013782972228597768 . Val loss: 0.022116117365658283 . Test loss: 1.637520558953748\n",
      "Epoch: 92 completed in: 13.231818675994873 s. Training loss: 0.01375677989265562 . Val loss: 0.0221255156211555 . Test loss: 1.633947945791857\n",
      "Epoch: 93 completed in: 13.320901155471802 s. Training loss: 0.013727429870135375 . Val loss: 0.022132279118523 . Test loss: 1.6378600367454625\n",
      "Epoch: 94 completed in: 13.275429964065552 s. Training loss: 0.013685546397319892 . Val loss: 0.02210558750666678 . Test loss: 1.6354371117600208\n",
      "Epoch: 95 completed in: 13.470628499984741 s. Training loss: 0.01364842489919197 . Val loss: 0.022094493312761188 . Test loss: 1.6385225704000606\n",
      "7.651428115381812e-05\n",
      "Epoch: 96 completed in: 13.253934383392334 s. Training loss: 0.013635707502491283 . Val loss: 0.022094081388786435 . Test loss: 1.6352297578322323\n",
      "Epoch: 97 completed in: 13.378797054290771 s. Training loss: 0.013610995010928299 . Val loss: 0.022094108816236257 . Test loss: 1.634487658319255\n",
      "Epoch: 98 completed in: 13.32338547706604 s. Training loss: 0.013579547955878894 . Val loss: 0.022087815729901195 . Test loss: 1.6367187303173931\n",
      "Epoch: 99 completed in: 13.88646650314331 s. Training loss: 0.013559634195654278 . Val loss: 0.02207706165499985 . Test loss: 1.6362233800144041\n",
      "Epoch: 100 completed in: 14.553285121917725 s. Training loss: 0.013543568602525187 . Val loss: 0.022092397045344114 . Test loss: 1.6366919890664506\n",
      "5.920529220333995e-05\n",
      "1.6366919890664506\n",
      "6938980\n",
      "------Trial 3\n",
      "Epoch: 1 completed in: 13.48597764968872 s. Training loss: 0.20735463534012763 . Val loss: 0.12835086975246668 . Test loss: 1.341268060152329\n",
      "Epoch: 2 completed in: 13.283495664596558 s. Training loss: 0.11300915275720784 . Val loss: 0.10477292854338885 . Test loss: 1.2576140626598784\n",
      "Epoch: 3 completed in: 13.259674787521362 s. Training loss: 0.09979931197135988 . Val loss: 0.10223326031118632 . Test loss: 1.287609225620918\n",
      "Epoch: 4 completed in: 13.277317762374878 s. Training loss: 0.09297621987053077 . Val loss: 0.10435465034097433 . Test loss: 1.2706072854554997\n",
      "Epoch: 5 completed in: 13.263868570327759 s. Training loss: 0.08867666631066887 . Val loss: 0.09232248682528735 . Test loss: 1.3661017970656684\n",
      "0.007737809374999999\n",
      "Epoch: 6 completed in: 13.307077884674072 s. Training loss: 0.08418976041032886 . Val loss: 0.08742515202611685 . Test loss: 1.2970121675526267\n",
      "Epoch: 7 completed in: 13.239987850189209 s. Training loss: 0.08000336054467806 . Val loss: 0.08268684912472964 . Test loss: 1.2645091635168686\n",
      "Epoch: 8 completed in: 13.41671085357666 s. Training loss: 0.07616996634615598 . Val loss: 0.07765072211623192 . Test loss: 1.298360709748879\n",
      "Epoch: 9 completed in: 13.295328378677368 s. Training loss: 0.07404271628789223 . Val loss: 0.07798661813139915 . Test loss: 1.294885413400331\n",
      "Epoch: 10 completed in: 13.295020341873169 s. Training loss: 0.07134181526183647 . Val loss: 0.07390865497291088 . Test loss: 1.333053948392339\n",
      "0.005987369392383786\n",
      "Epoch: 11 completed in: 13.380465030670166 s. Training loss: 0.06855109151806274 . Val loss: 0.0728722837753594 . Test loss: 1.3172749061590086\n",
      "Epoch: 12 completed in: 13.25824761390686 s. Training loss: 0.0653634319748104 . Val loss: 0.06795850694179535 . Test loss: 1.334251167764206\n",
      "Epoch: 13 completed in: 13.392066478729248 s. Training loss: 0.06256034925416873 . Val loss: 0.06945424191653729 . Test loss: 1.3068388898792125\n",
      "Epoch: 14 completed in: 13.333861351013184 s. Training loss: 0.06219974052786392 . Val loss: 0.06547626173123718 . Test loss: 1.3748915842176197\n",
      "Epoch: 15 completed in: 13.298418521881104 s. Training loss: 0.058843537988345115 . Val loss: 0.0629436650313437 . Test loss: 1.340595460763822\n",
      "0.00463291230159753\n",
      "Epoch: 16 completed in: 13.388813495635986 s. Training loss: 0.05729400909023128 . Val loss: 0.060516190249472857 . Test loss: 1.3434130284149308\n",
      "Epoch: 17 completed in: 13.32903528213501 s. Training loss: 0.05363091368255389 . Val loss: 0.05849336814135313 . Test loss: 1.3271153596431506\n",
      "Epoch: 18 completed in: 13.471924304962158 s. Training loss: 0.051169499686925954 . Val loss: 0.05527854030951858 . Test loss: 1.3897693738297128\n",
      "Epoch: 19 completed in: 13.46980881690979 s. Training loss: 0.04890239403250009 . Val loss: 0.05298830326646566 . Test loss: 1.3675829311126642\n",
      "Epoch: 20 completed in: 13.390592575073242 s. Training loss: 0.04631445713232468 . Val loss: 0.05010495996102691 . Test loss: 1.382925320380277\n",
      "0.0035848592240854188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 completed in: 13.384044408798218 s. Training loss: 0.04509551139262906 . Val loss: 0.049910007044672966 . Test loss: 1.3892015553878234\n",
      "Epoch: 22 completed in: 13.3307626247406 s. Training loss: 0.04279304478643802 . Val loss: 0.04752143518999219 . Test loss: 1.405409864821162\n",
      "Epoch: 23 completed in: 13.343639135360718 s. Training loss: 0.04085792038236221 . Val loss: 0.0460353490896523 . Test loss: 1.417949080527841\n",
      "Epoch: 24 completed in: 13.402570724487305 s. Training loss: 0.03941833497072658 . Val loss: 0.04370355186983943 . Test loss: 1.4133921570098118\n",
      "Epoch: 25 completed in: 13.374975681304932 s. Training loss: 0.0374443986432722 . Val loss: 0.042001869156956674 . Test loss: 1.4238003830172814\n",
      "0.0027738957312183374\n",
      "Epoch: 26 completed in: 13.425501585006714 s. Training loss: 0.03639352136719836 . Val loss: 0.04195376038551331 . Test loss: 1.4282160929062124\n",
      "Epoch: 27 completed in: 13.654013633728027 s. Training loss: 0.034812934166432295 . Val loss: 0.040405950788408515 . Test loss: 1.4406459204541084\n",
      "Epoch: 28 completed in: 13.470614194869995 s. Training loss: 0.03455976511684865 . Val loss: 0.03864894416183233 . Test loss: 1.4321017270993472\n",
      "Epoch: 29 completed in: 13.42310094833374 s. Training loss: 0.03217241229203931 . Val loss: 0.037393986620008944 . Test loss: 1.4560517091797698\n",
      "Epoch: 30 completed in: 13.462803602218628 s. Training loss: 0.03128896691041053 . Val loss: 0.037657948676496744 . Test loss: 1.421638611447113\n",
      "0.0021463876394293723\n",
      "Epoch: 31 completed in: 13.408336877822876 s. Training loss: 0.03051192467746726 . Val loss: 0.035157333686947824 . Test loss: 1.4476868225352264\n",
      "Epoch: 32 completed in: 13.558934926986694 s. Training loss: 0.029133859464395655 . Val loss: 0.03427605493925512 . Test loss: 1.446529345727414\n",
      "Epoch: 33 completed in: 13.644616603851318 s. Training loss: 0.028072062723447373 . Val loss: 0.033065938856452703 . Test loss: 1.4488038908884047\n",
      "Epoch: 34 completed in: 13.476260423660278 s. Training loss: 0.02726904552321147 . Val loss: 0.03253086511977017 . Test loss: 1.4411139043698935\n",
      "Epoch: 35 completed in: 13.487369537353516 s. Training loss: 0.02664446433747772 . Val loss: 0.03235356281511485 . Test loss: 1.4599799062459855\n",
      "0.0016608338398760713\n",
      "Epoch: 36 completed in: 13.44614315032959 s. Training loss: 0.02676881250184383 . Val loss: 0.03150093411095441 . Test loss: 1.4551032943231161\n",
      "Epoch: 37 completed in: 13.420465230941772 s. Training loss: 0.025227747402106322 . Val loss: 0.030384758999571205 . Test loss: 1.4729386487116864\n",
      "Epoch: 38 completed in: 13.426470279693604 s. Training loss: 0.024408932890824592 . Val loss: 0.02970183757133782 . Test loss: 1.4603949240876108\n",
      "Epoch: 39 completed in: 13.456420660018921 s. Training loss: 0.02360329450699535 . Val loss: 0.029172463994473218 . Test loss: 1.4715339887218009\n",
      "Epoch: 40 completed in: 13.46376371383667 s. Training loss: 0.02316411427582485 . Val loss: 0.028479915019124745 . Test loss: 1.477232874047071\n",
      "0.0012851215656510308\n",
      "Epoch: 41 completed in: 13.924215316772461 s. Training loss: 0.022601821881304256 . Val loss: 0.028215709514915942 . Test loss: 1.4730331631441231\n",
      "Epoch: 42 completed in: 13.390151500701904 s. Training loss: 0.022249409602615084 . Val loss: 0.0278304492123425 . Test loss: 1.4824920908590504\n",
      "Epoch: 43 completed in: 13.446523427963257 s. Training loss: 0.021673835056024965 . Val loss: 0.02719381912611425 . Test loss: 1.486310399406124\n",
      "Epoch: 44 completed in: 13.365339994430542 s. Training loss: 0.021206053844007262 . Val loss: 0.02688557715155184 . Test loss: 1.4770968351964673\n",
      "Epoch: 45 completed in: 13.39914608001709 s. Training loss: 0.02080987704523506 . Val loss: 0.026729684928432106 . Test loss: 1.4795524744834871\n",
      "0.000994402569870922\n",
      "Epoch: 46 completed in: 13.354482412338257 s. Training loss: 0.020654274307082603 . Val loss: 0.026284841634333134 . Test loss: 1.4747010280549218\n",
      "Epoch: 47 completed in: 13.530259847640991 s. Training loss: 0.020291885476640976 . Val loss: 0.02597183990292251 . Test loss: 1.4927214465843874\n",
      "Epoch: 48 completed in: 13.367560386657715 s. Training loss: 0.01975986829204281 . Val loss: 0.025692031998187304 . Test loss: 1.4906205841110245\n",
      "Epoch: 49 completed in: 13.361720085144043 s. Training loss: 0.01945261680011223 . Val loss: 0.025406729709357024 . Test loss: 1.4958079173047303\n",
      "Epoch: 50 completed in: 13.453607559204102 s. Training loss: 0.019153353153136526 . Val loss: 0.025127199618145822 . Test loss: 1.490102230246701\n",
      "0.0007694497527671312\n",
      "Epoch: 51 completed in: 13.28206992149353 s. Training loss: 0.018873200874204618 . Val loss: 0.024842235120013358 . Test loss: 1.4934300653790689\n",
      "Epoch: 52 completed in: 13.387251615524292 s. Training loss: 0.01856681162722572 . Val loss: 0.024614108679816126 . Test loss: 1.4983324155930215\n",
      "Epoch: 53 completed in: 13.255123376846313 s. Training loss: 0.018349077032511905 . Val loss: 0.02436831486411393 . Test loss: 1.4972845941139827\n",
      "Epoch: 54 completed in: 13.303898096084595 s. Training loss: 0.01810974284435493 . Val loss: 0.024206410441547633 . Test loss: 1.4972224630922306\n",
      "Epoch: 55 completed in: 13.316113948822021 s. Training loss: 0.01789690623821242 . Val loss: 0.024357455084100366 . Test loss: 1.5009798975701616\n",
      "0.000595385551055294\n",
      "Epoch: 56 completed in: 13.238755702972412 s. Training loss: 0.017705875479473467 . Val loss: 0.023993793176487088 . Test loss: 1.4936562320888607\n",
      "Epoch: 57 completed in: 13.243591547012329 s. Training loss: 0.01756980928871101 . Val loss: 0.023796955961734055 . Test loss: 1.4960534211390593\n",
      "Epoch: 58 completed in: 13.276422023773193 s. Training loss: 0.017280110644081432 . Val loss: 0.023609697120264173 . Test loss: 1.4946100865073226\n",
      "Epoch: 59 completed in: 13.234683275222778 s. Training loss: 0.017081596202006305 . Val loss: 0.023503920715302228 . Test loss: 1.502122389470379\n",
      "Epoch: 60 completed in: 13.244102716445923 s. Training loss: 0.016925483054896124 . Val loss: 0.02339693414978683 . Test loss: 1.5039478975190026\n",
      "0.00046069798986951934\n",
      "Epoch: 61 completed in: 13.202297449111938 s. Training loss: 0.016789473436881593 . Val loss: 0.023271495802327992 . Test loss: 1.502932647142217\n",
      "Epoch: 62 completed in: 13.243342638015747 s. Training loss: 0.016617413164952594 . Val loss: 0.023246887139976025 . Test loss: 1.50581656680171\n",
      "Epoch: 63 completed in: 13.264299392700195 s. Training loss: 0.01645078277895159 . Val loss: 0.023058829177170993 . Test loss: 1.502483420617923\n",
      "Epoch: 64 completed in: 13.187687873840332 s. Training loss: 0.016313987189264845 . Val loss: 0.02296835444867611 . Test loss: 1.5056271052836425\n",
      "Epoch: 65 completed in: 13.27700924873352 s. Training loss: 0.016197504139213014 . Val loss: 0.022944628121331336 . Test loss: 1.510117157083879\n",
      "0.00035647932250560207\n",
      "Epoch: 66 completed in: 13.269587755203247 s. Training loss: 0.01608026278524721 . Val loss: 0.02277824548073113 . Test loss: 1.5056765173600086\n",
      "Epoch: 67 completed in: 13.429842710494995 s. Training loss: 0.015965806731587127 . Val loss: 0.022838787501677872 . Test loss: 1.5125918954760693\n",
      "Epoch: 68 completed in: 13.24088430404663 s. Training loss: 0.015874825608338753 . Val loss: 0.02268587606959045 . Test loss: 1.5057122399025171\n",
      "Epoch: 69 completed in: 13.638015985488892 s. Training loss: 0.01575758675262876 . Val loss: 0.02262734863907099 . Test loss: 1.5100729186918511\n",
      "Epoch: 70 completed in: 13.386587858200073 s. Training loss: 0.015659559606472508 . Val loss: 0.0225693307351321 . Test loss: 1.5124297602274974\n",
      "0.00027583690436774953\n",
      "Epoch: 71 completed in: 13.362322092056274 s. Training loss: 0.01555152728908906 . Val loss: 0.02258427538909018 . Test loss: 1.5101453792436463\n",
      "Epoch: 72 completed in: 13.394474506378174 s. Training loss: 0.015478791315516416 . Val loss: 0.022477516578510402 . Test loss: 1.5120061523976285\n",
      "Epoch: 73 completed in: 13.356149435043335 s. Training loss: 0.01537033002891571 . Val loss: 0.022436434961855413 . Test loss: 1.5144292759004638\n",
      "Epoch: 74 completed in: 13.381609916687012 s. Training loss: 0.015303120438526146 . Val loss: 0.022422510851174592 . Test loss: 1.5124106658891527\n",
      "Epoch: 75 completed in: 13.306450366973877 s. Training loss: 0.015218349187260997 . Val loss: 0.02239416684024036 . Test loss: 1.512317314932056\n",
      "0.00021343733845877503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76 completed in: 13.43778944015503 s. Training loss: 0.015138648616924991 . Val loss: 0.022343089431524278 . Test loss: 1.5169422590789632\n",
      "Epoch: 77 completed in: 13.315095663070679 s. Training loss: 0.015098048353662892 . Val loss: 0.022288185730576515 . Test loss: 1.5132263263843209\n",
      "Epoch: 78 completed in: 13.346868991851807 s. Training loss: 0.01502702112248453 . Val loss: 0.022292685508728028 . Test loss: 1.5129464038387963\n",
      "Epoch: 79 completed in: 13.315927267074585 s. Training loss: 0.014957192754984772 . Val loss: 0.02222245940938592 . Test loss: 1.5141444130980601\n",
      "Epoch: 80 completed in: 13.34012770652771 s. Training loss: 0.014893501303600568 . Val loss: 0.022251472948119043 . Test loss: 1.5162538340523102\n",
      "0.00016515374385013573\n",
      "Epoch: 81 completed in: 13.323540687561035 s. Training loss: 0.014828821715565712 . Val loss: 0.02220911714248359 . Test loss: 1.5143486214602886\n",
      "Epoch: 82 completed in: 13.366432905197144 s. Training loss: 0.014772377976209578 . Val loss: 0.022170697106048465 . Test loss: 1.5150220667044179\n",
      "Epoch: 83 completed in: 13.247062921524048 s. Training loss: 0.014722610439724513 . Val loss: 0.022167300805449486 . Test loss: 1.5148068840843225\n",
      "Epoch: 84 completed in: 13.335880041122437 s. Training loss: 0.014661915250204122 . Val loss: 0.022141571482643486 . Test loss: 1.5165535009275362\n",
      "Epoch: 85 completed in: 13.269100427627563 s. Training loss: 0.01463642187525321 . Val loss: 0.022105056047439575 . Test loss: 1.516546591263481\n",
      "0.00012779281874799285\n",
      "Epoch: 86 completed in: 13.262472867965698 s. Training loss: 0.014583669626663853 . Val loss: 0.022102166991680862 . Test loss: 1.5167368823601484\n",
      "Epoch: 87 completed in: 13.37183141708374 s. Training loss: 0.014559059913684853 . Val loss: 0.02209526696242392 . Test loss: 1.516965360066954\n",
      "Epoch: 88 completed in: 13.55535888671875 s. Training loss: 0.014515678214765813 . Val loss: 0.022060640342533588 . Test loss: 1.5181997914998409\n",
      "Epoch: 89 completed in: 13.508941650390625 s. Training loss: 0.014489139627366171 . Val loss: 0.022059737285599113 . Test loss: 1.519122370114282\n",
      "Epoch: 90 completed in: 13.387792587280273 s. Training loss: 0.014424670596409887 . Val loss: 0.02204808979295194 . Test loss: 1.5177429225230539\n",
      "9.888364709658946e-05\n",
      "Epoch: 91 completed in: 13.325414419174194 s. Training loss: 0.014366647762239632 . Val loss: 0.02203431399539113 . Test loss: 1.5174248739738483\n",
      "Epoch: 92 completed in: 13.553975820541382 s. Training loss: 0.014360419760057091 . Val loss: 0.022033924562856556 . Test loss: 1.5170980947504784\n",
      "Epoch: 93 completed in: 13.342283964157104 s. Training loss: 0.0143195547836486 . Val loss: 0.022030594712123274 . Test loss: 1.5189953878909188\n",
      "Epoch: 94 completed in: 13.547298908233643 s. Training loss: 0.014303526140912607 . Val loss: 0.022016629204154016 . Test loss: 1.5179708366995803\n",
      "Epoch: 95 completed in: 13.606102466583252 s. Training loss: 0.014300413286979617 . Val loss: 0.0220138440374285 . Test loss: 1.5181250898355982\n",
      "7.651428115381812e-05\n",
      "Epoch: 96 completed in: 13.603514909744263 s. Training loss: 0.014245086604470972 . Val loss: 0.022004086570814253 . Test loss: 1.5188934748695082\n",
      "Epoch: 97 completed in: 13.519630908966064 s. Training loss: 0.014204027729421637 . Val loss: 0.021991009032353757 . Test loss: 1.5188520669700087\n",
      "Epoch: 98 completed in: 13.55505633354187 s. Training loss: 0.01419761736476182 . Val loss: 0.021983407158404588 . Test loss: 1.520167482929639\n",
      "Epoch: 99 completed in: 13.256028652191162 s. Training loss: 0.014171371783680506 . Val loss: 0.021973273390904068 . Test loss: 1.519899298310025\n",
      "Epoch: 100 completed in: 13.348034143447876 s. Training loss: 0.014135933338399352 . Val loss: 0.02198194875381887 . Test loss: 1.5193477581467276\n",
      "5.920529220333995e-05\n",
      "1.5193477581467276\n"
     ]
    }
   ],
   "source": [
    "# train model \n",
    "\n",
    "trial_num = 3 \n",
    "preds_total = [] \n",
    "test_losses = [] \n",
    "num_epoch = 100 \n",
    "\n",
    "for i in range(trial_num): \n",
    "    # build model \n",
    "    model = Seq2Seq_Attn(input_dim = 60, hidden_dim = 512, output_dim = 60, num_layers = 1, device = device).to(device) \n",
    "#     model = Seq2Seq(input_dim = 60, hidden_dim = 512, output_dim = 60, num_layers = 1).to(device) \n",
    "#     model = FC(input_dim = 60, input_len = 12, hidden_dim = 512, output_dim = 60).to(device) \n",
    "#     model = Latent_ODE(latent_dim = 256, obs_dim = 240, nhidden = 512, rhidden = 512, aug = False).to(device)\n",
    "    name = \"Seq2Seq_Attn\"\n",
    "    learning_rate = 0.01\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 1, gamma=0.95)\n",
    "    criterion = nn.MSELoss()\n",
    "    print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "    print(\"------Trial\", i + 1)\n",
    "    best_loss = 100   \n",
    "    train_losses = []\n",
    "    val_losses = [] \n",
    "    \n",
    "    for epoch in range(1, num_epoch + 1): \n",
    "        start = time.time()\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion)[-1]\n",
    "        train_losses.append(train_loss)\n",
    "        _, _, val_loss = eval_epoch(model, val_loader, criterion) \n",
    "        val_losses.append(val_loss)\n",
    "        if val_loss <= best_loss: \n",
    "            best_loss = val_loss \n",
    "            best_model = model \n",
    "#             torch.save({\"preds\": preds, \"trues\": trues, \"model\": best_model}, \"best_time_\" + name + str(i+1) + \".pt\") \n",
    "        end = time.time()\n",
    "        preds, trues, test_loss = eval_epoch_true(best_model, test_loader, criterion, std, mean) \n",
    "        print(\"Epoch:\", epoch, \"completed in:\", (end - start), \"s. Training loss:\", train_loss, \". Val loss:\", val_loss, \". Test loss:\", test_loss) \n",
    "        if (len(train_losses) > 50 and np.mean(val_losses[-5:]) >= np.mean(val_losses[-10:-5])):\n",
    "            break\n",
    "        scheduler.step() \n",
    "        if epoch % 5 == 0: print(optimizer.param_groups[0]['lr']) \n",
    "    \n",
    "    # save the best model, prediction and ground truth \n",
    "    preds, trues, test_loss = eval_epoch_true(best_model, test_loader, criterion, std, mean) \n",
    "    torch.save({\"preds\": preds, \"trues\": trues, \"model\": best_model}, \"result/Seq2Seq_Attn/best_space_region\" + name + str(i) + \".pt\") \n",
    "    preds_total.append(preds)\n",
    "    test_losses.append(test_loss)\n",
    "    print(test_loss)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c1aa78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, trues, test_loss = eval_epoch_true(best_model, test_loader, criterion, std, mean) \n",
    "torch.save({\"preds\": preds, \"trues\": trues, \"model\": best_model}, \"result_bay/Seq2Seq/best_time_\" + name + str(2) + \".pt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce002ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((1, 512, 1024))\n",
    "b = torch.zeros((1, 512, 1024))\n",
    "torch.cat((a, b), dim = 2).shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9901be",
   "metadata": {},
   "source": [
    "### Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e7407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_losses = []\n",
    "for preds_i in preds_total: \n",
    "#     print(preds_i.shape)\n",
    "    rmse_losses.append(torch.sqrt(criterion(torch.tensor(preds_i), torch.tensor(trues))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e6e518",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43846394",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(rmse_losses).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06e8f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensor location \n",
    "\n",
    "xi = [0,  45,  56,  75,  81,  86,  89,  95, 100, 105, 109, 112, 117,\n",
    "       124, 128, 133, 137, 141, 146, 149, 152, 158, 163, 167, 171, 174,\n",
    "       180, 186, 192, 197, 200, 205, 207, 210, 211, 213, 214, 228, 231,\n",
    "       237, 240, 242, 251, 254, 258, 262, 266, 270, 277, 279, 282, 283,\n",
    "       286, 288, 291, 294, 296, 298, 300, 303, 308, 310, 315, 317, 320,\n",
    "       322, 327, 338, 342, 345, 352, 356, 359, 362, 366, 368, 374, 379] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9041331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the last time step from the first test sample \n",
    "\n",
    "preds_t = np.stack(preds_total)\n",
    "preds_mean = preds_t.mean(axis = 0)\n",
    "gt = trues[0]\n",
    "loss = np.array(test_losses).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6406920",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_1 = preds_t.std(axis = 0)[-1, 0, -1]\n",
    "std_2 = preds_t.std(axis = 0)[-1, 1, -1]\n",
    "std_3 = preds_t.std(axis = 0)[-1, 2, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa21350",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = preds_mean[-1, :, -1, :]\n",
    "std = [std_1, std_2, std_3]\n",
    "true = trues[-1][:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01062548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tp=24\n",
    "plt.style.use('default')\n",
    "label={0: \"Density $k$ (veh/m)\",\n",
    "       1: \"Flow $q$ (veh/s)\",\n",
    "       2: \"Speed $u$ (m/s)\"} \n",
    "\n",
    "from matplotlib import gridspec\n",
    "\n",
    "fwyp=[xi[n]* 300 / 1.e3 for n in range(len(xi))]\n",
    "\n",
    "#gs = gridspec.GridSpec(3, 1, height_ratios=[2, 1, 1]) \n",
    "gs_kw={\"height_ratios\": [3, 3, 3]}\n",
    "\n",
    "fig4,ax4=plt.subplots(figsize=(5, 15), nrows=3, gridspec_kw=gs_kw, sharex=True)\n",
    "fig4.subplots_adjust(left=0.08, bottom=0.08, right=0.98, top=0.95)\n",
    "ymin=[-0.01, -0.1, 20]\n",
    "ymax=[0.06, 1.5, 40]\n",
    "\n",
    "\n",
    "for n in range(3): \n",
    "    ax4[n].plot(fwyp, (true[n, :]), label='observed', marker='o', markersize=5)\n",
    "    ax4[n].plot(fwyp, (result[n, :]), label='predicted', marker='o', markersize=5)\n",
    "    ax4[n].fill_between(fwyp, (result[n, :] - std[n]), (result[n, :] + std[n]),  color='red', alpha=.3)    \n",
    "    ax4[n].set_ylim(ymin[n], ymax[n])\n",
    "    #ax4[n].set_xlabel(\"Distance (grid points)\")\n",
    "    ax4[n].set_ylabel(label[n])\n",
    "\n",
    "    #ax4[n].set_xlim(fwyp[0]-1, fwyp[-1]+1)\n",
    "\n",
    "#     lastpoint=-100\n",
    "#     if n==0:\n",
    "#         for p in range(len(xi)):\n",
    "#             if (fwyp[p] - lastpoint) > 1.:\n",
    "#                 ax4[n].text(fwyp[p], (y_pred[n, :, tp-68])[p] + 0.02, \"none\", rotation=90, fontsize=8)\n",
    "#                 lastpoint=fwyp[p]\n",
    "ax4[0].set_title(\"FC: last timestep from the last test sample\")    \n",
    "ax4[2].set_xlabel(\"Relative distance from North to South (km)\")\n",
    "#ax4[n].set_ylabel(\"Traffic density (Veh/m)\")\n",
    "fig4.savefig('FC.jpg', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
